groups:
- name: kubernetes-resources
  rules:
  - alert: KubeCPUOvercommit
    annotations:
      message: Cluster has overcommitted CPU resource requests for pods and cannot tolerate node failure.
      runbook_url: https://docs.kubermatic.io/monitoring/runbook/#alert-kubecpuovercommit
    expr: |
      sum(namespace_name:kube_pod_container_resource_requests_cpu_cores:sum)
        /
      sum(node:node_num_cpu:sum)
        >
      (count(node:node_num_cpu:sum)-1) / count(node:node_num_cpu:sum)
    for: 5m
    labels:
      severity: warning

  - alert: KubeMemOvercommit
    annotations:
      message: Cluster has overcommitted memory resource requests for pods and cannot tolerate node failure.
      runbook_url: https://docs.kubermatic.io/monitoring/runbook/#alert-kubememovercommit
    expr: |
      sum(namespace_name:kube_pod_container_resource_requests_memory_bytes:sum)
        /
      sum(node_memory_MemTotal_bytes)
        >
      (count(node:node_num_cpu:sum)-1)
        /
      count(node:node_num_cpu:sum)
    for: 5m
    labels:
      severity: warning

  - alert: KubeCPUOvercommit
    annotations:
      message: Cluster has overcommitted CPU resource requests for namespaces.
      runbook_url: https://docs.kubermatic.io/monitoring/runbook/#alert-kubecpuovercommit
    expr: |
      sum(kube_resourcequota{job="kube-state-metrics", type="hard", resource="requests.cpu"})
        /
      sum(node:node_num_cpu:sum)
        > 1.5
    for: 5m
    labels:
      severity: warning

  - alert: KubeMemOvercommit
    annotations:
      message: Cluster has overcommitted memory resource requests for namespaces.
      runbook_url: https://docs.kubermatic.io/monitoring/runbook/#alert-kubememovercommit
    expr: |
      sum(kube_resourcequota{job="kube-state-metrics", type="hard", resource="requests.memory"})
        /
      sum(node_memory_MemTotal_bytes{app="node-exporter"})
        > 1.5
    for: 5m
    labels:
      severity: warning

  - alert: KubeQuotaExceeded
    annotations:
      message: Namespace {{ $labels.namespace }} is using {{ printf "%0.0f" $value }}% of its {{ $labels.resource }} quota.
      runbook_url: https://docs.kubermatic.io/monitoring/runbook/#alert-kubequotaexceeded
    expr: |
      100 * kube_resourcequota{job="kube-state-metrics", type="used"}
        / ignoring(instance, job, type)
      (kube_resourcequota{job="kube-state-metrics", type="hard"} > 0)
        > 90
    for: 15m
    labels:
      severity: warning

  # triggered by kernel bug, see issue kubermatic#2367

  # - alert: CPUThrottlingHigh
  #   annotations:
  #     message: '{{ printf "%0.0f" $value }}% throttling of CPU in namespace {{ $labels.namespace }} for {{ $labels.container_name }}.'
  #     runbook_url: https://docs.kubermatic.io/monitoring/runbook/#alert-cputhrottlinghigh
  #   expr: |
  #     100 * sum(increase(container_cpu_cfs_throttled_periods_total[5m])) by (container_name, pod_name, namespace)
  #       /
  #     sum(increase(container_cpu_cfs_periods_total[5m])) by (container_name, pod_name, namespace)
  #       > 25
  #   for: 15m
  #   labels:
  #     severity: warning

  - alert: KubePodOOMKilled
    annotations:
      message:
        Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }}
        has been OOMKilled {{ $value }} times in the last 30 minutes.
      runbook_url: https://docs.kubermatic.io/monitoring/runbook/#alert-kubepodoomkilled
    expr: |
      (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 30m >= 2)
      and
      ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[30m]) == 1
    for: 0m
    labels:
      severity: warning
