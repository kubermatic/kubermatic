# This file was generated by running `make rules`.
"groups":
- "name": "k8s.rules"
  "rules":
  - "expr": |
      sum(rate(container_cpu_usage_seconds_total{job="cadvisor", image!=""}[5m])) by (namespace)
    "record": "namespace:container_cpu_usage_seconds_total:sum_rate"
  - "expr": |
      sum(container_memory_usage_bytes{job="cadvisor", image!=""}) by (namespace)
    "record": "namespace:container_memory_usage_bytes:sum"
  - "expr": |
      sum by (namespace, label_name) (
         sum(rate(container_cpu_usage_seconds_total{job="cadvisor", image!=""}[5m])) by (namespace, pod_name)
       * on (namespace, pod_name) group_left(label_name)
         label_replace(kube_pod_labels{job="kube-state-metrics"}, "pod_name", "$1", "pod", "(.*)")
      )
    "record": "namespace_name:container_cpu_usage_seconds_total:sum_rate"
  - "expr": |
      sum by (namespace, label_name) (
        sum(container_memory_usage_bytes{job="cadvisor",image!=""}) by (pod_name, namespace)
      * on (namespace, pod_name) group_left(label_name)
        label_replace(kube_pod_labels{job="kube-state-metrics"}, "pod_name", "$1", "pod", "(.*)")
      )
    "record": "namespace_name:container_memory_usage_bytes:sum"
  - "expr": |
      sum by (namespace, label_name) (
        sum(kube_pod_container_resource_requests_memory_bytes{job="kube-state-metrics"}) by (namespace, pod)
      * on (namespace, pod) group_left(label_name)
        label_replace(kube_pod_labels{job="kube-state-metrics"}, "pod_name", "$1", "pod", "(.*)")
      )
    "record": "namespace_name:kube_pod_container_resource_requests_memory_bytes:sum"
  - "expr": |
      sum by (namespace, label_name) (
        sum(kube_pod_container_resource_requests_cpu_cores{job="kube-state-metrics"} and on(pod) kube_pod_status_scheduled{condition="true"}) by (namespace, pod)
      * on (namespace, pod) group_left(label_name)
        label_replace(kube_pod_labels{job="kube-state-metrics"}, "pod_name", "$1", "pod", "(.*)")
      )
    "record": "namespace_name:kube_pod_container_resource_requests_cpu_cores:sum"
- "name": "kube-scheduler.rules"
  "rules":
  - "expr": |
      histogram_quantile(0.99, sum(rate(scheduler_e2e_scheduling_latency_microseconds_bucket{job="scheduler"}[5m])) without(instance, pod)) / 1e+06
    "labels":
      "quantile": "0.99"
    "record": "cluster_quantile:scheduler_e2e_scheduling_latency:histogram_quantile"
  - "expr": |
      histogram_quantile(0.99, sum(rate(scheduler_scheduling_algorithm_latency_microseconds_bucket{job="scheduler"}[5m])) without(instance, pod)) / 1e+06
    "labels":
      "quantile": "0.99"
    "record": "cluster_quantile:scheduler_scheduling_algorithm_latency:histogram_quantile"
  - "expr": |
      histogram_quantile(0.99, sum(rate(scheduler_binding_latency_microseconds_bucket{job="scheduler"}[5m])) without(instance, pod)) / 1e+06
    "labels":
      "quantile": "0.99"
    "record": "cluster_quantile:scheduler_binding_latency:histogram_quantile"
  - "expr": |
      histogram_quantile(0.9, sum(rate(scheduler_e2e_scheduling_latency_microseconds_bucket{job="scheduler"}[5m])) without(instance, pod)) / 1e+06
    "labels":
      "quantile": "0.9"
    "record": "cluster_quantile:scheduler_e2e_scheduling_latency:histogram_quantile"
  - "expr": |
      histogram_quantile(0.9, sum(rate(scheduler_scheduling_algorithm_latency_microseconds_bucket{job="scheduler"}[5m])) without(instance, pod)) / 1e+06
    "labels":
      "quantile": "0.9"
    "record": "cluster_quantile:scheduler_scheduling_algorithm_latency:histogram_quantile"
  - "expr": |
      histogram_quantile(0.9, sum(rate(scheduler_binding_latency_microseconds_bucket{job="scheduler"}[5m])) without(instance, pod)) / 1e+06
    "labels":
      "quantile": "0.9"
    "record": "cluster_quantile:scheduler_binding_latency:histogram_quantile"
  - "expr": |
      histogram_quantile(0.5, sum(rate(scheduler_e2e_scheduling_latency_microseconds_bucket{job="scheduler"}[5m])) without(instance, pod)) / 1e+06
    "labels":
      "quantile": "0.5"
    "record": "cluster_quantile:scheduler_e2e_scheduling_latency:histogram_quantile"
  - "expr": |
      histogram_quantile(0.5, sum(rate(scheduler_scheduling_algorithm_latency_microseconds_bucket{job="scheduler"}[5m])) without(instance, pod)) / 1e+06
    "labels":
      "quantile": "0.5"
    "record": "cluster_quantile:scheduler_scheduling_algorithm_latency:histogram_quantile"
  - "expr": |
      histogram_quantile(0.5, sum(rate(scheduler_binding_latency_microseconds_bucket{job="scheduler"}[5m])) without(instance, pod)) / 1e+06
    "labels":
      "quantile": "0.5"
    "record": "cluster_quantile:scheduler_binding_latency:histogram_quantile"
- "name": "kube-apiserver.rules"
  "rules":
  - "expr": |
      histogram_quantile(0.99, sum(rate(apiserver_request_latencies_bucket{job="apiserver"}[5m])) without(instance, pod)) / 1e+06
    "labels":
      "quantile": "0.99"
    "record": "cluster_quantile:apiserver_request_latencies:histogram_quantile"
  - "expr": |
      histogram_quantile(0.9, sum(rate(apiserver_request_latencies_bucket{job="apiserver"}[5m])) without(instance, pod)) / 1e+06
    "labels":
      "quantile": "0.9"
    "record": "cluster_quantile:apiserver_request_latencies:histogram_quantile"
  - "expr": |
      histogram_quantile(0.5, sum(rate(apiserver_request_latencies_bucket{job="apiserver"}[5m])) without(instance, pod)) / 1e+06
    "labels":
      "quantile": "0.5"
    "record": "cluster_quantile:apiserver_request_latencies:histogram_quantile"
- "name": "node.rules"
  "rules":
  - "expr": "sum(min(kube_pod_info) by (node))"
    "record": ":kube_pod_info_node_count:"
  - "expr": |
      max(label_replace(kube_pod_info{job="kube-state-metrics"}, "pod", "$1", "pod", "(.*)")) by (node, namespace, pod)
    "record": "node_namespace_pod:kube_pod_info:"
  - "expr": |
      count by (node) (sum by (node, cpu) (
        node_cpu{app="node-exporter"}
      * on (namespace, pod) group_left(node)
        node_namespace_pod:kube_pod_info:
      ))
    "record": "node:node_num_cpu:sum"
  - "expr": |
      1 - avg(rate(node_cpu{app="node-exporter",mode="idle"}[1m]))
    "record": ":node_cpu_utilisation:avg1m"
  - "expr": |
      1 - avg by (node) (
        rate(node_cpu{app="node-exporter",mode="idle"}[1m])
      * on (namespace, pod) group_left(node)
        node_namespace_pod:kube_pod_info:)
    "record": "node:node_cpu_utilisation:avg1m"
  - "expr": |
      sum(node_load1{app="node-exporter"})
      /
      sum(node:node_num_cpu:sum)
    "record": ":node_cpu_saturation_load1:"
  - "expr": |
      sum by (node) (
        node_load1{app="node-exporter"}
      * on (namespace, pod) group_left(node)
        node_namespace_pod:kube_pod_info:
      )
      /
      node:node_num_cpu:sum
    "record": "node:node_cpu_saturation_load1:"
  - "expr": |
      1 -
      sum(node_memory_MemFree{app="node-exporter"} + node_memory_Cached{app="node-exporter"} + node_memory_Buffers{app="node-exporter"})
      /
      sum(node_memory_MemTotal{app="node-exporter"})
    "record": ":node_memory_utilisation:"
  - "expr": |
      sum by (node) (
        (node_memory_MemFree{app="node-exporter"} + node_memory_Cached{app="node-exporter"} + node_memory_Buffers{app="node-exporter"})
        * on (namespace, pod) group_left(node)
          node_namespace_pod:kube_pod_info:
      )
    "record": "node:node_memory_bytes_available:sum"
  - "expr": |
      sum by (node) (
        node_memory_MemTotal{app="node-exporter"}
        * on (namespace, pod) group_left(node)
          node_namespace_pod:kube_pod_info:
      )
    "record": "node:node_memory_bytes_total:sum"
  - "expr": |
      (node:node_memory_bytes_total:sum - node:node_memory_bytes_available:sum)
      /
      scalar(sum(node:node_memory_bytes_total:sum))
    "record": "node:node_memory_utilisation:ratio"
  - "expr": |
      1e3 * sum(
        (rate(node_vmstat_pgpgin{app="node-exporter"}[1m])
       + rate(node_vmstat_pgpgout{app="node-exporter"}[1m]))
      )
    "record": ":node_memory_swap_io_bytes:sum_rate"
  - "expr": |
      1 -
      sum by (node) (
        (node_memory_MemFree{app="node-exporter"} + node_memory_Cached{app="node-exporter"} + node_memory_Buffers{app="node-exporter"})
      * on (namespace, pod) group_left(node)
        node_namespace_pod:kube_pod_info:
      )
      /
      sum by (node) (
        node_memory_MemTotal{app="node-exporter"}
      * on (namespace, pod) group_left(node)
        node_namespace_pod:kube_pod_info:
      )
    "record": "node:node_memory_utilisation:"
  - "expr": |
      1 - (node:node_memory_bytes_available:sum / node:node_memory_bytes_total:sum)
    "record": "node:node_memory_utilisation_2:"
  - "expr": |
      1e3 * sum by (node) (
        (rate(node_vmstat_pgpgin{app="node-exporter"}[1m])
       + rate(node_vmstat_pgpgout{app="node-exporter"}[1m]))
       * on (namespace, pod) group_left(node)
         node_namespace_pod:kube_pod_info:
      )
    "record": "node:node_memory_swap_io_bytes:sum_rate"
  - "expr": |
      avg(irate(node_disk_io_time_ms{app="node-exporter",device=~"(sd|xvd|nvme).+"}[1m]) / 1e3)
    "record": ":node_disk_utilisation:avg_irate"
  - "expr": |
      avg by (node) (
        irate(node_disk_io_time_ms{app="node-exporter",device=~"(sd|xvd|nvme).+"}[1m]) / 1e3
      * on (namespace, pod) group_left(node)
        node_namespace_pod:kube_pod_info:
      )
    "record": "node:node_disk_utilisation:avg_irate"
  - "expr": |
      avg(irate(node_disk_io_time_weighted{app="node-exporter",device=~"(sd|xvd|nvme).+"}[1m]) / 1e3)
    "record": ":node_disk_saturation:avg_irate"
  - "expr": |
      avg by (node) (
        irate(node_disk_io_time_weighted{app="node-exporter",device=~"(sd|xvd|nvme).+"}[1m]) / 1e3
      * on (namespace, pod) group_left(node)
        node_namespace_pod:kube_pod_info:
      )
    "record": "node:node_disk_saturation:avg_irate"
  - "expr": |
      sum(irate(node_network_receive_bytes{app="node-exporter",device="eth0"}[1m])) +
      sum(irate(node_network_transmit_bytes{app="node-exporter",device="eth0"}[1m]))
    "record": ":node_net_utilisation:sum_irate"
  - "expr": |
      sum by (node) (
        (irate(node_network_receive_bytes{app="node-exporter",device="eth0"}[1m]) +
        irate(node_network_transmit_bytes{app="node-exporter",device="eth0"}[1m]))
      * on (namespace, pod) group_left(node)
        node_namespace_pod:kube_pod_info:
      )
    "record": "node:node_net_utilisation:sum_irate"
  - "expr": |
      sum(irate(node_network_receive_drop{app="node-exporter",device="eth0"}[1m])) +
      sum(irate(node_network_transmit_drop{app="node-exporter",device="eth0"}[1m]))
    "record": ":node_net_saturation:sum_irate"
  - "expr": |
      sum by (node) (
        (irate(node_network_receive_drop{app="node-exporter",device="eth0"}[1m]) +
        irate(node_network_transmit_drop{app="node-exporter",device="eth0"}[1m]))
      * on (namespace, pod) group_left(node)
        node_namespace_pod:kube_pod_info:
      )
    "record": "node:node_net_saturation:sum_irate"
- "name": "node-exporter.rules"
  "rules":
  - "expr": |
      count by (instance) (
        sum by (instance, cpu) (
          node_cpu{app="node-exporter"}
        )
      )
    "record": "instance:node_num_cpu:sum"
  - "expr": |
      1 - avg by (instance) (
        rate(node_cpu{app="node-exporter",mode="idle"}[1m])
      )
    "record": "instance:node_cpu_utilisation:avg1m"
  - "expr": |
      sum by (instance) (node_load1{app="node-exporter"})
      /
      instance:node_num_cpu:sum
    "record": "instance:node_cpu_saturation_load1:"
  - "expr": |
      sum by (instance) (
        node_memory_MemTotal{app="node-exporter"}
      )
    "record": "instance:node_memory_bytes_total:sum"
  - "expr": |
      1 - (
          node_memory_MemAvailable{app="node-exporter"}
        /
          node_memory_MemTotal{app="node-exporter"}
      )
    "record": "instance:node_memory_utilisation:ratio"
  - "expr": |
      1e3 * sum by (instance) (
        (rate(node_vmstat_pgpgin{app="node-exporter"}[1m])
         + rate(node_vmstat_pgpgout{app="node-exporter"}[1m]))
      )
    "record": "instance:node_memory_swap_io_bytes:sum_rate"
  - "expr": |
      sum by (instance) (
        irate(node_disk_io_time_ms{app="node-exporter",device=~"(sd|xvd).+"}[1m]) / 1e3
      )
    "record": "instance:node_disk_utilisation:sum_irate"
  - "expr": |
      sum by (instance) (
        irate(node_disk_io_time_weighted{app="node-exporter",device=~"(sd|xvd).+"}[1m]) / 1e3
      )
    "record": "instance:node_disk_saturation:sum_irate"
  - "expr": |
      sum by (instance) (
        (irate(node_network_receive_bytes{app="node-exporter",device=~"eth[0-9]+"}[1m]) +
         irate(node_network_transmit_bytes{app="node-exporter",device=~"eth[0-9]+"}[1m]))
      )
    "record": "instance:node_net_utilisation:sum_irate"
  - "expr": |
      sum by (instance) (
        (irate(node_network_receive_drop{app="node-exporter",device=~"eth[0-9]+"}[1m]) +
         irate(node_network_transmit_drop{app="node-exporter",device=~"eth[0-9]+"}[1m]))
      )
    "record": "instance:node_net_saturation:sum_irate"
- "name": "kubermatic"
  "rules":
  - "alert": "KubermaticTooManyUnhandledErrors"
    "annotations":
      "message": "Kubermatic controller manager in {{ $labels.namespace }} is experiencing too many errors."
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubermatictoomanyunhandlederrors"
    "expr": |
      sum(rate(kubermatic_controller_manager_unhandled_errors_total[5m])) > 0.01
    "for": "10m"
    "labels":
      "severity": "warning"
  - "alert": "KubermaticClusterDeletionTakesTooLong"
    "annotations":
      "message": "Cluster {{ $labels.cluster }} is stuck in deletion for more than 30min."
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubermaticclusterdeletiontakestoolong"
    "expr": "(time() - kubermatic_cluster_deleted) > 30*60"
    "for": "0m"
    "labels":
      "severity": "warning"
- "name": "kubernetes-absent"
  "rules":
  - "alert": "CadvisorDown"
    "annotations":
      "message": "Cadvisor has disappeared from Prometheus target discovery."
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-cadvisordown"
    "expr": |
      absent(up{job="cadvisor"} == 1)
    "for": "15m"
    "labels":
      "severity": "critical"
  - "alert": "KubeAPIDown"
    "annotations":
      "message": "KubeAPI has disappeared from Prometheus target discovery."
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubeapidown"
    "expr": |
      absent(up{job="apiserver"} == 1)
    "for": "15m"
    "labels":
      "severity": "critical"
  - "alert": "KubeControllerManagerDown"
    "annotations":
      "message": "KubeControllerManager has disappeared from Prometheus target discovery."
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubecontrollermanagerdown"
    "expr": |
      absent(up{job="controller-manager"} == 1)
    "for": "15m"
    "labels":
      "severity": "critical"
  - "alert": "KubeSchedulerDown"
    "annotations":
      "message": "KubeScheduler has disappeared from Prometheus target discovery."
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubeschedulerdown"
    "expr": |
      absent(up{job="scheduler"} == 1)
    "for": "15m"
    "labels":
      "severity": "critical"
  - "alert": "KubeStateMetricsDown"
    "annotations":
      "message": "KubeStateMetrics has disappeared from Prometheus target discovery."
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubestatemetricsdown"
    "expr": |
      absent(up{job="kube-state-metrics"} == 1)
    "for": "15m"
    "labels":
      "severity": "critical"
  - "alert": "KubeletDown"
    "annotations":
      "message": "Kubelet has disappeared from Prometheus target discovery."
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubeletdown"
    "expr": |
      absent(up{job="kubelet"} == 1)
    "for": "15m"
    "labels":
      "severity": "critical"
  - "alert": "KubermaticAPIDown"
    "annotations":
      "message": "KubermaticAPI has disappeared from Prometheus target discovery."
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubermaticapidown"
    "expr": |
      absent(up{job="pods",namespace="kubermatic",role="kubermatic-api"} == 1)
    "for": "15m"
    "labels":
      "severity": "critical"
  - "alert": "KubermaticControllerManagerDown"
    "annotations":
      "message": "KubermaticControllerManager has disappeared from Prometheus target discovery."
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubermaticcontrollermanagerdown"
    "expr": |
      absent(up{job="pods",namespace="kubermatic",role="controller-manager"} == 1)
    "for": "15m"
    "labels":
      "severity": "critical"
  - "alert": "KubernetesApiserverDown"
    "annotations":
      "message": "KubernetesApiserver has disappeared from Prometheus target discovery."
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubernetesapiserverdown"
    "expr": |
      absent(up{job="apiserver"} == 1)
    "for": "15m"
    "labels":
      "severity": "critical"
- "name": "kubernetes-apps"
  "rules":
  - "alert": "KubePodCrashLooping"
    "annotations":
      "message": "{{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is restarting {{ printf \"%.2f\" $value }} / second"
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubepodcrashlooping"
    "expr": |
      rate(kube_pod_container_status_restarts_total{job="kube-state-metrics"}[15m]) > 0
    "for": "1h"
    "labels":
      "severity": "critical"
  - "alert": "KubePodNotReady"
    "annotations":
      "message": "{{ $labels.namespace }}/{{ $labels.pod }} is not ready."
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubepodnotready"
    "expr": |
      sum by (namespace, pod) (kube_pod_status_phase{job="kube-state-metrics", phase!~"Running|Succeeded"}) > 0
    "for": "1h"
    "labels":
      "severity": "critical"
  - "alert": "KubeDeploymentGenerationMismatch"
    "annotations":
      "message": "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} generation mismatch"
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubedeploymentgenerationmismatch"
    "expr": |
      kube_deployment_status_observed_generation{job="kube-state-metrics"}
        !=
      kube_deployment_metadata_generation{job="kube-state-metrics"}
    "for": "15m"
    "labels":
      "severity": "critical"
  - "alert": "KubeDeploymentReplicasMismatch"
    "annotations":
      "message": "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} replica mismatch"
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubedeploymentreplicasmismatch"
    "expr": |
      kube_deployment_spec_replicas{job="kube-state-metrics"}
        !=
      kube_deployment_status_replicas_available{job="kube-state-metrics"}
    "for": "1h"
    "labels":
      "severity": "critical"
  - "alert": "KubeStatefulSetReplicasMismatch"
    "annotations":
      "message": "StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} replica mismatch"
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubestatefulsetreplicasmismatch"
    "expr": |
      kube_statefulset_status_replicas_ready{job="kube-state-metrics"}
        !=
      kube_statefulset_status_replicas{job="kube-state-metrics"}
    "for": "15m"
    "labels":
      "severity": "critical"
  - "alert": "KubeStatefulSetGenerationMismatch"
    "annotations":
      "message": "StatefulSet {{ $labels.namespace }}/{{ labels.statefulset }} generation mismatch"
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubestatefulsetgenerationmismatch"
    "expr": |
      kube_statefulset_status_observed_generation{job="kube-state-metrics"}
        !=
      kube_statefulset_metadata_generation{job="kube-state-metrics"}
    "for": "15m"
    "labels":
      "severity": "critical"
  - "alert": "KubeDaemonSetRolloutStuck"
    "annotations":
      "message": "Only {{$value}}% of desired pods scheduled and ready for daemon set {{$labels.namespace}}/{{$labels.daemonset}}"
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubedaemonsetrolloutstuck"
    "expr": |
      kube_daemonset_status_number_ready{job="kube-state-metrics"}
        /
      kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"} * 100 < 100
    "for": "15m"
    "labels":
      "severity": "critical"
  - "alert": "KubeDaemonSetNotScheduled"
    "annotations":
      "message": "A number of pods of daemonset {{$labels.namespace}}/{{$labels.daemonset}} are not scheduled."
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubedaemonsetnotscheduled"
    "expr": |
      kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"}
        -
      kube_daemonset_status_current_number_scheduled{job="kube-state-metrics"} > 0
    "for": "10m"
    "labels":
      "severity": "warning"
  - "alert": "KubeDaemonSetMisScheduled"
    "annotations":
      "message": "A number of pods of daemonset {{$labels.namespace}}/{{$labels.daemonset}} are running where they are not supposed to run."
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubedaemonsetmisscheduled"
    "expr": |
      kube_daemonset_status_number_misscheduled{job="kube-state-metrics"} > 0
    "for": "10m"
    "labels":
      "severity": "warning"
  - "alert": "KubeCronJobRunning"
    "annotations":
      "message": "CronJob {{ $labels.namespaces }}/{{ $labels.cronjob }} is taking more than 1h to complete."
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubecronjobrunning"
    "expr": |
      time() - kube_cronjob_next_schedule_time{job="kube-state-metrics"} > 3600
    "for": "1h"
    "labels":
      "severity": "warning"
  - "alert": "KubeJobCompletion"
    "annotations":
      "message": "Job {{ $labels.namespaces }}/{{ $labels.job }} is taking more than 1h to complete."
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubejobcompletion"
    "expr": |
      kube_job_spec_completions{job="kube-state-metrics"} - kube_job_status_succeeded{job="kube-state-metrics"}  > 0
    "for": "1h"
    "labels":
      "severity": "warning"
  - "alert": "KubeJobFailed"
    "annotations":
      "message": "Job {{ $labels.namespaces }}/{{ $labels.job }} failed to complete."
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubejobfailed"
    "expr": |
      kube_job_status_failed{job="kube-state-metrics"}  > 0
    "for": "1h"
    "labels":
      "severity": "warning"
- "name": "kubernetes-resources"
  "rules":
  - "alert": "KubeCPUOvercommit"
    "annotations":
      "message": "Overcommited CPU resource requests on Pods, cannot tolerate node failure."
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubecpuovercommit"
    "expr": |
      sum(namespace_name:kube_pod_container_resource_requests_cpu_cores:sum)
        /
      sum(node:node_num_cpu:sum)
        >
      (count(node:node_num_cpu:sum)-1) / count(node:node_num_cpu:sum)
    "for": "5m"
    "labels":
      "severity": "warning"
  - "alert": "KubeMemOvercommit"
    "annotations":
      "message": "Overcommited Memory resource requests on Pods, cannot tolerate node failure."
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubememovercommit"
    "expr": |
      sum(namespace_name:kube_pod_container_resource_requests_memory_bytes:sum)
        /
      sum(node_memory_MemTotal)
        >
      (count(node:node_num_cpu:sum)-1)
        /
      count(node:node_num_cpu:sum)
    "for": "5m"
    "labels":
      "severity": "warning"
  - "alert": "KubeCPUOvercommit"
    "annotations":
      "message": "Overcommited CPU resource request quota on Namespaces."
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubecpuovercommit"
    "expr": |
      sum(kube_resourcequota{job="kube-state-metrics", type="hard", resource="requests.cpu"})
        /
      sum(node:node_num_cpu:sum)
        > 1.5
    "for": "5m"
    "labels":
      "severity": "warning"
  - "alert": "KubeMemOvercommit"
    "annotations":
      "message": "Overcommited Memory resource request quota on Namespaces."
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubememovercommit"
    "expr": |
      sum(kube_resourcequota{job="kube-state-metrics", type="hard", resource="requests.memory"})
        /
      sum(node_memory_MemTotal{app="node-exporter"})
        > 1.5
    "for": "5m"
    "labels":
      "severity": "warning"
  - "alert": "KubeQuotaExceeded"
    "annotations":
      "message": "{{ printf \"%0.0f\" $value }}% usage of {{ $labels.resource }} in namespace {{ $labels.namespace }}."
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubequotaexceeded"
    "expr": |
      100 * kube_resourcequota{job="kube-state-metrics", type="used"}
        / ignoring(instance, job, type)
      kube_resourcequota{job="kube-state-metrics", type="hard"}
        > 90
    "for": "15m"
    "labels":
      "severity": "warning"
- "name": "kubernetes-storage"
  "rules":
  - "alert": "KubePersistentVolumeUsageCritical"
    "annotations":
      "message": "The persistent volume claimed by {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} has {{ printf \"%0.0f\" $value }}% free."
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubepersistentvolumeusagecritical"
    "expr": |
      100 * kubelet_volume_stats_available_bytes{job="kubelet"}
        /
      kubelet_volume_stats_capacity_bytes{job="kubelet"}
        < 3
    "for": "1m"
    "labels":
      "severity": "critical"
  - "alert": "KubePersistentVolumeFullInFourDays"
    "annotations":
      "message": "Based on recent sampling, the persistent volume claimed by {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} is expected to fill up within four days."
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubepersistentvolumefullinfourdays"
    "expr": |
      predict_linear(kubelet_volume_stats_available_bytes{job="kubelet"}[1h], 4 * 24 * 3600) < 0
    "for": "5m"
    "labels":
      "severity": "critical"
- "name": "kubernetes-system"
  "rules":
  - "alert": "KubeNodeNotReady"
    "annotations":
      "message": "{{ $labels.node }} has been unready for more than an hour"
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubenodenotready"
    "expr": |
      kube_node_status_condition{job="kube-state-metrics",condition="Ready",status="true"} == 0
    "for": "1h"
    "labels":
      "severity": "warning"
  - "alert": "KubeVersionMismatch"
    "annotations":
      "message": "There are {{ $value }} different versions of Kubernetes components running."
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubeversionmismatch"
    "expr": |
      count(count(kubernetes_build_info{job!="dns"}) by (gitVersion)) > 1
    "for": "1h"
    "labels":
      "severity": "warning"
  - "alert": "KubeClientErrors"
    "annotations":
      "message": "Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance }}' is experiencing {{ printf \"%0.0f\" $value }}% errors.'"
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubeclienterrors"
    "expr": |
      sum(rate(rest_client_requests_total{code!~"2.."}[5m])) by (instance, job) * 100
        /
      sum(rate(rest_client_requests_total[5m])) by (instance, job)
        > 1
    "for": "15m"
    "labels":
      "severity": "warning"
  - "alert": "KubeClientErrors"
    "annotations":
      "message": "Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance }}' is experiencing {{ printf \"%0.0f\" $value }} errors / sec.'"
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubeclienterrors"
    "expr": |
      sum(rate(ksm_scrape_error_total{job="kube-state-metrics"}[5m])) by (instance, job) > 0.1
    "for": "15m"
    "labels":
      "severity": "warning"
  - "alert": "KubeletTooManyPods"
    "annotations":
      "message": "Kubelet {{$labels.instance}} is running {{$value}} pods, close to the limit of 110."
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubelettoomanypods"
    "expr": |
      kubelet_running_pod_count{job="kubelet"} > 100
    "for": "15m"
    "labels":
      "severity": "warning"
  - "alert": "KubeAPILatencyHigh"
    "annotations":
      "message": "The API server has a 99th percentile latency of {{ $value }} seconds for {{$labels.verb}} {{$labels.resource}}."
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubeapilatencyhigh"
    "expr": |
      cluster_quantile:apiserver_request_latencies:histogram_quantile{job="apiserver",quantile="0.99",subresource!="log",verb!~"^(?:WATCH|WATCHLIST|PROXY|CONNECT)$"} > 1
    "for": "10m"
    "labels":
      "severity": "warning"
  - "alert": "KubeAPILatencyHigh"
    "annotations":
      "message": "The API server has a 99th percentile latency of {{ $value }} seconds for {{$labels.verb}} {{$labels.resource}}."
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubeapilatencyhigh"
    "expr": |
      cluster_quantile:apiserver_request_latencies:histogram_quantile{job="apiserver",quantile="0.99",subresource!="log",verb!~"^(?:WATCH|WATCHLIST|PROXY|CONNECT)$"} > 4
    "for": "10m"
    "labels":
      "severity": "critical"
  - "alert": "KubeAPIErrorsHigh"
    "annotations":
      "message": "API server is erroring for {{ $value }}% of requests."
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubeapierrorshigh"
    "expr": |
      sum(rate(apiserver_request_count{job="apiserver",code=~"^(?:5..)$"}[5m])) without(instance, pod)
        /
      sum(rate(apiserver_request_count{job="apiserver"}[5m])) without(instance, pod) * 100 > 5
    "for": "10m"
    "labels":
      "severity": "critical"
  - "alert": "KubeAPIErrorsHigh"
    "annotations":
      "message": "API server is erroring for {{ $value }}% of requests."
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubeapierrorshigh"
    "expr": |
      sum(rate(apiserver_request_count{job="apiserver",code=~"^(?:5..)$"}[5m])) without(instance, pod)
        /
      sum(rate(apiserver_request_count{job="apiserver"}[5m])) without(instance, pod) * 100 > 5
    "for": "10m"
    "labels":
      "severity": "warning"
  - "alert": "KubeClientCertificateExpiration"
    "annotations":
      "message": "Kubernetes API certificate is expiring in less than 7 days."
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubeclientcertificateexpiration"
    "expr": |
      histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 604800
    "labels":
      "severity": "warning"
  - "alert": "KubeClientCertificateExpiration"
    "annotations":
      "message": "Kubernetes API certificate is expiring in less than 1 day."
      "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubeclientcertificateexpiration"
    "expr": |
      histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 86400
    "labels":
      "severity": "critical"
- "name": "prometheus"
  "rules":
  - "alert": "PromScrapeFailed"
    "annotations":
      "message": "Prometheus failed to scrape a target {{ $labels.job }} / {{ $labels.instance }}"
    "expr": |
      up != 1
    "for": "15m"
    "labels":
      "severity": "warning"
  - "alert": "PromBadConfig"
    "annotations":
      "mesage": "Prometheus failed to reload config, see container logs"
    "expr": |
      prometheus_config_last_reload_successful{job="prometheus"} == 0
    "for": "15m"
    "labels":
      "severity": "critical"
  - "alert": "PromAlertmanagerBadConfig"
    "annotations":
      "message": "Alertmanager failed to reload config, see container logs"
    "expr": |
      alertmanager_config_last_reload_successful{job="alertmanager"} == 0
    "for": "10m"
    "labels":
      "severity": "critical"
  - "alert": "PromAlertsFailed"
    "annotations":
      "message": "Alertmanager failed to send an alert."
    "expr": |
      sum(increase(alertmanager_notifications_failed_total{job="alertmanager"}[5m])) by (namespace) > 0
    "for": "5m"
    "labels":
      "severity": "critical"
  - "alert": "PromRemoteStorageFailures"
    "annotations":
      "message": "Prometheus failed to send {{ printf \"%.1f\" $value }}% samples"
    "expr": |
      (rate(prometheus_remote_storage_failed_samples_total{job="prometheus"}[1m]) * 100)
        /
      (rate(prometheus_remote_storage_failed_samples_total{job="prometheus"}[1m]) + rate(prometheus_remote_storage_succeeded_samples_total{job="prometheus"}[1m]))
        > 1
    "for": "15m"
    "labels":
      "severity": "critical"
  - "alert": "PromRuleFailures"
    "annotations":
      "message": "Prometheus failed to evaluate {{ printf \"%.1f\" $value }} rules / s"
    "expr": |
      rate(prometheus_rule_evaluation_failures_total{job="prometheus"}[1m]) > 0
    "for": "15m"
    "labels":
      "severity": "critical"
- "name": "node-exporter"
  "rules":
  - "alert": "NodeFilesystemSpaceFillingUp"
    "annotations":
      "message": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} is predicted to run out of space within the next 24 hours."
    "expr": |
      predict_linear(node_filesystem_avail{app="node-exporter",fstype=~"ext.|xfs",mountpoint!="/var/lib/docker/aufs"}[6h], 24*60*60) < 0
      and
      node_filesystem_avail{app="node-exporter",fstype=~"ext.|xfs",mountpoint!="/var/lib/docker/aufs"} / node_filesystem_size{app="node-exporter",fstype=~"ext.|xfs",mountpoint!="/var/lib/docker/aufs"} < 0.4
      and
      node_filesystem_readonly{app="node-exporter",fstype=~"ext.|xfs",mountpoint!="/var/lib/docker/aufs"} == 0
    "for": "1h"
    "labels":
      "severity": "warning"
  - "alert": "NodeFilesystemSpaceFillingUp"
    "annotations":
      "message": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} is predicted to run out of space within the next 4 hours."
    "expr": |
      predict_linear(node_filesystem_avail{app="node-exporter",fstype=~"ext.|xfs",mountpoint!="/var/lib/docker/aufs"}[6h], 4*60*60) < 0
      and
      node_filesystem_avail{app="node-exporter",fstype=~"ext.|xfs",mountpoint!="/var/lib/docker/aufs"} / node_filesystem_size{app="node-exporter",fstype=~"ext.|xfs",mountpoint!="/var/lib/docker/aufs"} < 0.2
      and
      node_filesystem_readonly{app="node-exporter",fstype=~"ext.|xfs",mountpoint!="/var/lib/docker/aufs"} == 0
    "for": "1h"
    "labels":
      "severity": "critical"
  - "alert": "NodeFilesystemOutOfSpace"
    "annotations":
      "message": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ $value }}% available space left."
    "expr": |
      node_filesystem_avail{app="node-exporter",fstype=~"ext.|xfs",mountpoint!="/var/lib/docker/aufs"} / node_filesystem_size{app="node-exporter",fstype=~"ext.|xfs",mountpoint!="/var/lib/docker/aufs"} * 100 < 5
      and
      node_filesystem_readonly{app="node-exporter",fstype=~"ext.|xfs",mountpoint!="/var/lib/docker/aufs"} == 0
    "for": "1h"
    "labels":
      "severity": "warning"
  - "alert": "NodeFilesystemOutOfSpace"
    "annotations":
      "message": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ $value }}% available space left."
    "expr": |
      node_filesystem_avail{app="node-exporter",fstype=~"ext.|xfs",mountpoint!="/var/lib/docker/aufs"} / node_filesystem_size{app="node-exporter",fstype=~"ext.|xfs",mountpoint!="/var/lib/docker/aufs"} * 100 < 3
      and
      node_filesystem_readonly{app="node-exporter",fstype=~"ext.|xfs",mountpoint!="/var/lib/docker/aufs"} == 0
    "for": "1h"
    "labels":
      "severity": "critical"
  - "alert": "NodeFilesystemFilesFillingUp"
    "annotations":
      "message": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} is predicted to run out of files within the next 24 hours."
    "expr": |
      predict_linear(node_filesystem_files_free{app="node-exporter",fstype=~"ext.|xfs",mountpoint!="/var/lib/docker/aufs"}[6h], 24*60*60) < 0
      and
      node_filesystem_files_free{app="node-exporter",fstype=~"ext.|xfs",mountpoint!="/var/lib/docker/aufs"} / node_filesystem_files{app="node-exporter",fstype=~"ext.|xfs",mountpoint!="/var/lib/docker/aufs"} < 0.4
      and
      node_filesystem_readonly{app="node-exporter",fstype=~"ext.|xfs",mountpoint!="/var/lib/docker/aufs"} == 0
    "for": "1h"
    "labels":
      "severity": "warning"
  - "alert": "NodeFilesystemFilesFillingUp"
    "annotations":
      "message": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} is predicted to run out of files within the next 4 hours."
    "expr": |
      predict_linear(node_filesystem_files_free{app="node-exporter",fstype=~"ext.|xfs",mountpoint!="/var/lib/docker/aufs"}[6h], 4*60*60) < 0
      and
      node_filesystem_files_free{app="node-exporter",fstype=~"ext.|xfs",mountpoint!="/var/lib/docker/aufs"} / node_filesystem_files{app="node-exporter",fstype=~"ext.|xfs",mountpoint!="/var/lib/docker/aufs"} < 0.2
      and
      node_filesystem_readonly{app="node-exporter",fstype=~"ext.|xfs",mountpoint!="/var/lib/docker/aufs"} == 0
    "for": "1h"
    "labels":
      "severity": "warning"
  - "alert": "NodeFilesystemOutOfFiles"
    "annotations":
      "message": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ $value }}% available inodes left."
    "expr": |
      node_filesystem_files_free{app="node-exporter",fstype=~"ext.|xfs",mountpoint!="/var/lib/docker/aufs"} / node_filesystem_files{app="node-exporter",fstype=~"ext.|xfs",mountpoint!="/var/lib/docker/aufs"} * 100 < 5
      and
      node_filesystem_readonly{app="node-exporter",fstype=~"ext.|xfs",mountpoint!="/var/lib/docker/aufs"} == 0
    "for": "1h"
    "labels":
      "severity": "warning"
  - "alert": "NodeFilesystemOutOfSpace"
    "annotations":
      "message": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ $value }}% available space left."
    "expr": |
      node_filesystem_files_free{app="node-exporter",fstype=~"ext.|xfs",mountpoint!="/var/lib/docker/aufs"} / node_filesystem_files{app="node-exporter",fstype=~"ext.|xfs",mountpoint!="/var/lib/docker/aufs"} * 100 < 3
      and
      node_filesystem_readonly{app="node-exporter",fstype=~"ext.|xfs",mountpoint!="/var/lib/docker/aufs"} == 0
    "for": "1h"
    "labels":
      "severity": "critical"
  - "alert": "NodeNetworkReceiveErrs"
    "annotations":
      "message": "{{ $labels.instance }} interface {{ $labels.device }} shows errors while receiving packets ({{ $value }} errors in two minutes)."
    "expr": |
      increase(node_network_receive_errs[2m]) > 10
    "for": "1h"
    "labels":
      "severity": "critical"
  - "alert": "NodeNetworkTransmitErrs"
    "annotations":
      "message": "{{ $labels.instance }} interface {{ $labels.device }} shows errors while transmitting packets ({{ $value }} errors in two minutes)."
    "expr": |
      increase(node_network_transmit_errs[2m]) > 10
    "for": "1h"
    "labels":
      "severity": "critical"
