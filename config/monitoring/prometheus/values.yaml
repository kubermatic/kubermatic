prometheus:
  version: v2.9.2
  host: ''
  storageSize: 100Gi

  backup:
    enabled: true
    image:
      repository: quay.io/kubermatic/util
      tag: 1.0.0-2
    timeout: 60m

  # Specify additional external labels which will be added to all
  # alerts sent by Prometheus.
  #externalLabels:
  #  seed_cluster: default

  # Configure the scraping rules for Prometheus. You can either
  # add your own scraping configs here or change the path to the
  # predefined config files that are evaluated when Helm builds
  # the chart and deploys it. You cannot use this to load files
  # at runtime from a custom volume because Prometheus does not
  # support it.
  scraping:
    files:
    - config/scraping/*.yaml

    #configs:
    #- job_name: myscrapejob
    #  honor_labels: true
    #  ...

  # Similarly to the scraping config, you can configure the
  # target alertmanagers here.
  alertmanagers:
    files:
    - config/alertmanagers/*.yaml
    #configs:
    #- scheme: http
    #  path_prefix: /
    #  ...

  # The list of rule files to load; if you use the `volumes`
  # directive below to mount your own ConfigMap or Secret into
  # Prometheus, you will want to extend this list to laod your
  # own rule files. You can remove the predefined path to
  # effectively disable the stock recordings and alerts.
  ruleFiles:
  - /etc/prometheus/rules/general-*.yaml
  - /etc/prometheus/rules/kubermatic-master-*.yaml
  - /etc/prometheus/rules/kubermatic-seed-*.yaml
  # If you run in an environment where access to Kubernetes
  # scheduler and controller-manager is not possible (like GKE),
  # disable the expression below to not create false alerts
  # for missing/unhealthy components.
  - /etc/prometheus/rules/managed-*.yaml

  # Optionally add some more recording/alerting rules; the structure
  # beneath `rules` is identical to regular rules files as documented
  # in https://prometheus.io/docs/prometheus/2.7/getting_started/
  # For larger collections of rules, consider using the custom volume
  # approach shown further down in the `volumes` section.
  #rules:
  #  groups:
  #    - name: myrules
  #      rules:
  #      - alert: DatacenterIsOnFire
  #        expr: temperature{cpu} > 100
  #        for: 5m

  # If you prefer to manage your recording/alerting rules in your
  # own ConfigMaps or Secrets, you can use this section to mount
  # those into the Prometheus pods. Remember to extend the `ruleFiles`
  # section above to have your files be loaded into Prometheus.
  # For each volume, specify either a configMap name or a secretName,
  # never both.
  #volumes:
  #- name: initech-alerting-rules
  #  mountPath: /initech/alerts
  #  configMap: initech-alerting-rules-configmap
  #- name: initech-recording-rules
  #  mountPath: /initech/recordings
  #  secretName: initech-recording-rules-secret

  containers:
    prometheus:
      resources:
        requests:
          cpu: 1
          memory: 2Gi
        limits:
          cpu: 2
          memory: 10Gi
    backup:
      resources:
        requests:
          cpu: 100m
          memory: 64Mi
        limits:
          cpu: 500m
          memory: 10Gi
    reloader:
      resources:
        requests:
          cpu: 5m
          memory: 24Mi
        limits:
          cpu: 5m
          memory: 32Mi

  nodeSelector: {}
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchLabels:
              app: prometheus-kubermatic
          topologyKey: kubernetes.io/hostname
        weight: 100
  tolerations: []
