kubermatic:
  # Whether the cluster is a master cluster
  # This must be false for any non-master seed
  isMaster: true
  # the base64 encoded docker/quay authentication json file
  imagePullSecretData: ""
  auth:
    # the full path to the openid connect token issuer. For example 'https://dev.kubermatic.io/dex'
    tokenIssuer: ""
    # the client id for openid connect
    clientID: ""
    # skip tls verification on the token issuer
    skipTokenIssuerTLSVerify: "false"
  # base64 encoded datacenters.yaml
  datacenters: ""
  # external domain for the kubermatic installation. For example 'dev.kubermatic.io'
  domain: ""
  # base64 encoded kubeconfig which gives admin access to all seed clusters
  kubeconfig: ""
  # The prefix for monitoring annotations in the user cluster. Default: monitoring.kubermatic.io -> monitoring.kubermatic.io/scrape, monitoring.kubermatic.io/path
  monitoringScrapeAnnotationPrefix: ""
  deployVPA: true

  # helm hooks/checks
  checks:
    # Checks if the last release contains the kubermatic CRD's.
    # We moved them out of the chart to avoid issues with helm
    crd:
      disable: false
      helmVersion: "v2.11.0"

  etcd:
    # PV size for the etcd StatefulSet of new clusters
    diskSize: "5Gi"
  controller:
    # Available feature gates:
    # - OpenIDAuthPlugin
    #   If enabled configures the flags on the API server to use OAuth2 identity providers.
    # - VerticalPodAutoscaler
    #   If enabled the cluster-controller will enable the VerticalPodAutoscaler for all control plane components
    # For example:
    # featureGates: "OpenIDAuthPlugin=true,VerticalPodAutoscaler=true"
    featureGates: ""
    datacenterName: ""
    # Specifies the NodePort range for customer clusters - this must match the NodePort range of the seed cluster.
    nodeportRange: "30000-32767"
    replicas: 2
    image:
      repository: "quay.io/kubermatic/api"
      tag: "__KUBERMATIC_TAG__"
      pullPolicy: "IfNotPresent"
    addons:
      image:
        repository: "quay.io/kubermatic/addons"
        tag: "v0.1.16"
        pullPolicy: "IfNotPresent"
      # list of Addons to install into every user-cluster. All need to exist in the addons image
      defaultAddons:
      - canal
      - dashboard
      - dns
      - kube-proxy
      - openvpn
      - rbac
      - kubelet-configmap
      - default-storage-class
  api:
    replicas: 2
    image:
      repository: "quay.io/kubermatic/api"
      tag: "__KUBERMATIC_TAG__"
      pullPolicy: "IfNotPresent"
  ui:
    replicas: 2
    image:
      repository: "quay.io/kubermatic/ui-v2"
      tag: "v1.1.0"
      pullPolicy: "IfNotPresent"
    config: |
      {
        "share_kubeconfig": false,
        "show_demo_info": false,
        "show_terms_of_service": false,
        "cleanup_cluster": false
      }
  rbac:
    replicas: 1
    image:
      repository: "quay.io/kubermatic/api"
      tag: "__KUBERMATIC_TAG__"
      pullPolicy: "IfNotPresent"
  storeContainer: |
    command:
    - /bin/sh
    - -c
    - |
      set -euo pipefail
      s3-storeuploader store --endpoint minio.minio.svc.cluster.local:9000 --bucket kubermatic-etcd-backups --create-bucket --prefix $CLUSTER --file /backup/snapshot.db
      s3-storeuploader delete-old-revisions --endpoint minio.minio.svc.cluster.local:9000 --bucket kubermatic-etcd-backups --prefix $CLUSTER --file /backup/snapshot.db --max-revisions 20
    image: quay.io/kubermatic/s3-storer:v0.1.4
    name: store-container
    env:
    - name: ACCESS_KEY_ID
      valueFrom:
        secretKeyRef:
          name: s3-credentials
          key: ACCESS_KEY_ID
    - name: SECRET_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          name: s3-credentials
          key: SECRET_ACCESS_KEY
    volumeMounts:
    - name: etcd-backup
      mountPath: /backup
  cleanupContainer: |
    command:
    - /bin/sh
    - -c
    - |
      set -euo pipefail
      s3-storeuploader delete-all --endpoint minio.minio.svc.cluster.local:9000 --bucket kubermatic-etcd-backups --prefix $CLUSTER
    image: quay.io/kubermatic/s3-storer:v0.1.4
    name: cleanup-container
    env:
    - name: ACCESS_KEY_ID
      valueFrom:
        secretKeyRef:
          name: s3-credentials
          key: ACCESS_KEY_ID
    - name: SECRET_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          name: s3-credentials
          key: SECRET_ACCESS_KEY
#  clusterNamespacePrometheus:
#    disableDefaultScrapingConfigs: true
#    scrapingConfigs:
#    - job_name: 'schnitzel'
#      kubernetes_sd_configs:
#      - role: pod
#      relabel_configs:
#      - source_labels: [__meta_kubernetes_pod_annotation_kubermatic_scrape]
#        action: keep
#        regex: true
#    disableDefaultRules: false
#    rules:
#      groups:
#      - name: my-custom-group
#        rules:
#        - alert: MyCustomAlert
#          annotations:
#            message: Something happend in {{ $labels.namespace }}
#          expr: |
#            sum(rate(machine_controller_errors_total[5m])) by (namespace) > 0.01
#          for: 10m
#          labels:
#            severity: warning
  clusterNamespacePrometheus: {}
  vpa:
    updater:
      image:
        repository: k8s.gcr.io/vpa-updater
        tag: 0.3.0
    recommender:
      image:
        repository: k8s.gcr.io/vpa-recommender
        tag: 0.3.0
    admissioncontroller:
      image:
        repository: k8s.gcr.io/vpa-admission-controller
        tag: 0.3.0
