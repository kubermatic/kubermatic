kubermatic:
  # Whether the cluster is a master cluster
  # This must be false for any non-master seed
  isMaster: true
  # the base64 encoded docker/quay authentication json file
  imagePullSecretData: ""
  auth:
    # the full path to the openid connect token issuer. For example 'https://dev.kubermatic.io/dex'
    tokenIssuer: ""
    # the client id for openid connect
    clientID: ""
    # skip tls verification on the token issuer
    skipTokenIssuerTLSVerify: "false"
    # the service account signing key. Must be 32 bytes or longer
    serviceAccountKey: ""
  # base64 encoded datacenters.yaml
  datacenters: ""
  # external domain for the kubermatic installation. For example 'dev.kubermatic.io'
  domain: ""
  # base64 encoded kubeconfig which gives admin access to all seed clusters
  kubeconfig: ""
  # The prefix for monitoring annotations in the user cluster. Default: monitoring.kubermatic.io -> monitoring.kubermatic.io/scrape, monitoring.kubermatic.io/path
  monitoringScrapeAnnotationPrefix: ""
  # The location from which to pull the Kubermatic docker image
  kubermaticImage: ""
  # The location from which to pull the Kubermatic dnatcontroller image
  dnatcontrollerImage: "quay.io/kubermatic/kubeletdnat-controller"
  # The strategy to expose the cluster with, either "NodePort" which creates a NodePort with a "nodeport-proxy.k8s.io/expose": "true" annotation to expose all
  # clusters on one central Service of type LoadBalancer via the NodePort proxy or "LoadBalancer" to create a LoadBalancer service per cluster
  # **Note:** The `seed_dns_overwrite` setting of the `datacenters.yaml` doesn't have any effect if this is set to `LoadBalancer`
  exposeStrategy: "NodePort"
  # base64 encoded presets.yaml. Predefined presets for all supported providers.
  presets: ""

  # The default number of replicas for controlplane components. Can be overriden on
  # a per-cluster basis by setting .Spec.ComponentsOverride.$COMPONENT.Replicas
  apiserverDefaultReplicas: "2"
  controllerManagerDefaultReplicas: "1"
  schedulerDefaultReplicas: "1"
  maxParallelReconcile: "10"

  # Whether to disable reconciling for the apiserver endpoints
  apiserverEndpointReconcilingDisabled: false

  # Whether to load the datacenters from CRDs dynamically during runtime
  dynamicDatacenters: false

  # Whether to load the presets from CRDs dynamically during runtime
  dynamicPresets: false

  # Whether to enabled or disable default addon enforce option
  disableAddonEnforce: true

  # helm hooks/checks
  checks:
    # Checks if the last release contains the kubermatic CRD's.
    # We moved them out of the chart to avoid issues with helm
    crd:
      disable: false
      helmVersion: "v2.11.0"
      image:
        repository: "quay.io/kubermatic/util"
        tag: "1.3.2"

  etcd:
    # PV size for the etcd StatefulSet of new clusters
    diskSize: "5Gi"

  controller:
    # Available feature gates:
    # - OpenIDAuthPlugin
    #   If enabled configures the flags on the API server to use OAuth2 identity providers.
    # - VerticalPodAutoscaler
    #   If enabled the cluster-controller will enable the VerticalPodAutoscaler for all control plane components
    # - EtcdDataCorruptionChecks
    #   If enabled the all etcd clusters will be started with --experimental-initial-corrupt-check=true --experimental-corrupt-check-time=10m
    # For example:
    # featureGates: "OpenIDAuthPlugin=true,VerticalPodAutoscaler=true"
    featureGates: ""
    datacenterName: ""
    # Specifies the NodePort range for customer clusters - this must match the NodePort range of the seed cluster.
    nodeportRange: "30000-32767"
    replicas: 2
    image:
      repository: "quay.io/kubermatic/api"
      tag: "__KUBERMATIC_TAG__"
      pullPolicy: "IfNotPresent"
    pprofEndpoint: ":6600"
    addons:
      kubernetes:
        # list of Addons to install into every user-cluster. All need to exist in the addons image
        defaultAddons:
        - canal
        - csi
        - dns
        - kube-proxy
        - openvpn
        - rbac
        - kubelet-configmap
        - default-storage-class
        - nodelocal-dns-cache
        - pod-security-policy
        - logrotate
        image:
          repository: "quay.io/kubermatic/addons"
          tag: "__KUBERMATIC_TAG__"
          pullPolicy: "IfNotPresent"
      openshift:
        # Mutually exclusive with `defaultAddons`. The file referenced here is in the static/master/ folder
        defaultAddonsFile: openshift-addons.yaml
        image:
          repository: "quay.io/kubermatic/openshift-addons"
          tag: "__KUBERMATIC_TAG__"
          pullPolicy: "IfNotPresent"
    # Specify a custom docker registry which will be used for all images (user cluster control plane + addons)
    overwriteRegistry: ""
    resources:
      requests:
        cpu: 200m
        memory: 512Mi
      limits:
        cpu: 500m
        memory: 1Gi
    workerCount: 4
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          preference:
            matchExpressions:
            - key: kubermatic.io/type
              operator: In
              values:
              - stable
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                role: controller-manager
            topologyKey: kubernetes.io/hostname
          weight: 10
    nodeSelector: {}
    tolerations:
    - key: only_critical
      operator: Equal
      value: "true"
      effect: NoSchedule

  api:
    replicas: 2
    # List of optional addons that can be installed into every user-cluster. All need to exist in the addons image.
    accessibleAddons:
    - node-exporter
    image:
      repository: "quay.io/kubermatic/api"
      tag: "__KUBERMATIC_TAG__"
      pullPolicy: "IfNotPresent"
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 250m
        memory: 256Mi
    pprofEndpoint: ":6600"
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          preference:
            matchExpressions:
            - key: kubermatic.io/type
              operator: In
              values:
              - stable
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                role: kubermatic-api
            topologyKey: kubernetes.io/hostname
          weight: 10
    nodeSelector: {}
    tolerations:
    - key: only_critical
      operator: Equal
      value: "true"
      effect: NoSchedule

  ui:
    replicas: 2
    image:
      repository: "quay.io/kubermatic/ui-v2"
      tag: "__DASHBOARD_TAG__"
      pullPolicy: "IfNotPresent"
    # Config options for the dashboard:
    # share_kubeconfig: Specify if the button for "Share Kubeconfig" is visible.
    # oidc_provider_url: Change the base URL of the OIDC provider (BASE_URL).
    # oidc_provider_scope: Change the scope of the OIDC provider (SCOPE).
    config: |
      {
        "share_kubeconfig": false
      }
    resources:
      requests:
        cpu: 10m
        memory: 32Mi
      limits:
        cpu: 100m
        memory: 32Mi
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          preference:
            matchExpressions:
            - key: kubermatic.io/type
              operator: In
              values:
              - stable
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                role: kubermatic-ui
            topologyKey: kubernetes.io/hostname
          weight: 10
    nodeSelector: {}
    tolerations:
    - key: only_critical
      operator: Equal
      value: "true"
      effect: NoSchedule

  masterController:
    replicas: 1
    image:
      repository: quay.io/kubermatic/api
      tag: "__KUBERMATIC_TAG__"
      pullPolicy: IfNotPresent
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        cpu: 100m
        memory: 256Mi
    debugLog: false
    pprofEndpoint: ":6600"
    workerCount: 20
    affinity: {}
    nodeSelector: {}
    tolerations: []

  storeContainer: |
    command:
    - /bin/sh
    - -c
    - |
      set -euo pipefail
      s3-storeuploader store --endpoint minio.minio.svc.cluster.local:9000 --bucket kubermatic-etcd-backups --create-bucket --prefix $CLUSTER --file /backup/snapshot.db
      s3-storeuploader delete-old-revisions --endpoint minio.minio.svc.cluster.local:9000 --bucket kubermatic-etcd-backups --prefix $CLUSTER --file /backup/snapshot.db --max-revisions 20
    image: quay.io/kubermatic/s3-storer:v0.1.4
    name: store-container
    env:
    - name: ACCESS_KEY_ID
      valueFrom:
        secretKeyRef:
          name: s3-credentials
          key: ACCESS_KEY_ID
    - name: SECRET_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          name: s3-credentials
          key: SECRET_ACCESS_KEY
    volumeMounts:
    - name: etcd-backup
      mountPath: /backup

  cleanupContainer: |
    command:
    - /bin/sh
    - -c
    - |
      set -euo pipefail
      s3-storeuploader delete-all --endpoint minio.minio.svc.cluster.local:9000 --bucket kubermatic-etcd-backups --prefix $CLUSTER
    image: quay.io/kubermatic/s3-storer:v0.1.4
    name: cleanup-container
    env:
    - name: ACCESS_KEY_ID
      valueFrom:
        secretKeyRef:
          name: s3-credentials
          key: ACCESS_KEY_ID
    - name: SECRET_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          name: s3-credentials
          key: SECRET_ACCESS_KEY

  clusterNamespacePrometheus: {}
#  clusterNamespacePrometheus:
#    disableDefaultScrapingConfigs: true
#    scrapingConfigs:
#    - job_name: 'schnitzel'
#      kubernetes_sd_configs:
#      - role: pod
#      relabel_configs:
#      - source_labels: [__meta_kubernetes_pod_annotation_kubermatic_scrape]
#        action: keep
#        regex: true
#    disableDefaultRules: false
#    rules:
#      groups:
#      - name: my-custom-group
#        rules:
#        - alert: MyCustomAlert
#          annotations:
#            message: Something happend in {{ $labels.namespace }}
#          expr: |
#            sum(rate(machine_controller_errors_total[5m])) by (namespace) > 0.01
#          for: 10m
#          labels:
#            severity: warning

  vpa:
    updater:
      image:
        repository: gcr.io/google_containers/vpa-updater
        tag: 0.5.0
      resources:
        requests:
          cpu: 50m
          memory: 32Mi
        limits:
          cpu: 200m
          memory: 128Mi
      affinity: {}
      nodeSelector: {}
      tolerations: []

    recommender:
      image:
        repository: gcr.io/google_containers/vpa-recommender
        tag: 0.5.0
      resources:
        requests:
          cpu: 50m
          memory: 500Mi
        limits:
          cpu: 200m
          memory: 3000Mi
      affinity: {}
      nodeSelector: {}
      tolerations: []

    admissioncontroller:
      image:
        repository: gcr.io/google_containers/vpa-admission-controller
        tag: 0.5.0
      resources:
        requests:
          cpu: 50m
          memory: 32Mi
        limits:
          cpu: 200m
          memory: 128Mi
      affinity: {}
      nodeSelector: {}
      tolerations: []
