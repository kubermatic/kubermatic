logging:
  elasticsearch:
    image:
      repository: docker.elastic.co/elasticsearch/elasticsearch-oss
      tag: "6.8.5"
      pullPolicy: IfNotPresent

    cluster:
      # make sure to always configure the JVM to have min=max heap size, or else
      # Elasticsearch will refuse to start up.
      additionalJavaOpts: "-XX:MaxRAMPercentage=70 -XX:InitialRAMPercentage=70"
      config: {}

      # environment variables used on all data and master nodes;
      # note that MINIMUM_MASTER_NODES is computed automatically and does not need
      # to be set here
      env: {}

    # When the sum of master and data nodes is 1, the chart will deploy a cluster
    # with single-node mode enabled. In this mode there is no discovery and no
    # separation between masters and data nodes anymore. You also lose any redundancy
    # and should not use this for production workloads.
    # Note that you should set data=1 and master=0 if you need a single-node cluster.
    # Scaling the StatefulSets via kubectl will *not change the mode*, so to scale
    # up you need to re-deploy the chart with updated replica counts. Once the
    # regular cluster mode is enabled, using kubectl to scale up and down is fine.

    master:
      replicas: 3
      # additionalJavaOpts: ""
      resources:
        requests:
          cpu: 200m
          memory: 512Mi
        limits:
          cpu: 1
          memory: 1536Mi
      nodeSelector: {}
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  component: elasticsearch
                  role: master
              topologyKey: kubernetes.io/hostname
      tolerations: []
      storageSize: 5Gi

    data:
      replicas: 3
      # additionalJavaOpts: ""
      resources:
        requests:
          cpu: 200m
          memory: 2560Mi
        limits:
          cpu: 1
          memory: 4Gi
      nodeSelector: {}
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  component: elasticsearch
                  role: data
              topologyKey: kubernetes.io/hostname
      tolerations: []
      storageSize: 50Gi

    curator:
      # Amount of days after which the indicies should be killed
      interval: 5
      image:
        repository: quay.io/kubermatic/elasticsearch-curator
        tag: "5.7.4-1"
        pullPolicy: IfNotPresent

    init:
      image:
        repository: docker.io/library/busybox
        tag: "1.30.1"
        pullPolicy: IfNotPresent

    exporter:
      image:
        repository: docker.io/justwatch/elasticsearch_exporter
        tag: "1.1.0rc1"
        pullPolicy: IfNotPresent

      # see https://github.com/justwatchcom/elasticsearch_exporter#configuration
      all: true
      indices: true
      indices_settings: true
      shards: true

      resources:
        requests:
          cpu: 50m
          memory: 32Mi
        limits:
          cpu: 100m
          memory: 128Mi

      nodeSelector: {}
      affinity: {}
      tolerations: []

    cerebro:
      image:
        repository: docker.io/lmenezes/cerebro
        tag: "0.8.3"
        pullPolicy: IfNotPresent
      deploy: false
      resources:
        requests:
          cpu: 100m
          memory: 400Mi
        limits:
          cpu: 200m
          # memory usage spikes when starting
          memory: 800Mi
      nodeSelector: {}
      affinity: {}
      tolerations: []
