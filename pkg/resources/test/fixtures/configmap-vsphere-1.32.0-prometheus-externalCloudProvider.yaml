# This file has been generated, DO NOT EDIT.

data:
  prometheus.yaml: "global:\n  evaluation_interval: 30s\n  scrape_interval: 30s\n
    \ external_labels:\n    cluster: \"de-test-01\"\n    seed_cluster: \"test-seed\"\n\nrule_files:\n-
    \"/etc/prometheus/config/rules*.yaml\"\n\nalerting:\n  alertmanagers:\n  - dns_sd_configs:\n
    \   # configure the Seed's alertmanager for the user cluster\n    - names:\n      -
    'alertmanager.monitoring.svc.cluster.local'\n      type: A\n      port: 9093\n\nscrape_configs:\n#######################################################################\n#
    These rules will scrape pods running inside the seed cluster.\n\n# scrape the
    etcd pods\n- job_name: etcd\n  scheme: https\n  tls_config:\n    ca_file: /etc/etcd/pki/client/ca.crt\n
    \   cert_file: /etc/etcd/pki/client/apiserver-etcd-client.crt\n    key_file: /etc/etcd/pki/client/apiserver-etcd-client.key\n\n
    \ static_configs:\n  - targets:\n    - 'etcd-0.etcd.cluster-de-test-01.svc.cluster.local:2379'\n
    \   - 'etcd-1.etcd.cluster-de-test-01.svc.cluster.local:2379'\n    - 'etcd-2.etcd.cluster-de-test-01.svc.cluster.local:2379'\n\n
    \ relabel_configs:\n  - source_labels: [__address__]\n    regex: (etcd-\\d+).+\n
    \   action: replace\n    replacement: $1\n    target_label: instance\n\n# scrape
    the cluster's control plane (apiserver, controller-manager, scheduler)\n- job_name:
    kubernetes-control-plane\n  scheme: https\n  tls_config:\n    ca_file: /etc/kubernetes/ca.crt\n
    \   cert_file: /etc/kubernetes/prometheus-client.crt\n    key_file: /etc/kubernetes/prometheus-client.key\n
    \   # insecure_skip_verify is needed because the apiservers certificate\n    #
    does not contain a common name for the pod's ip address\n    insecure_skip_verify:
    true\n\n  kubernetes_sd_configs:\n  - role: pod\n    namespaces:\n      names:\n
    \     - \"cluster-de-test-01\"\n\n  relabel_configs:\n  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape_with_kube_cert]\n
    \   action: keep\n    regex: true\n  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\n
    \   action: replace\n    target_label: __metrics_path__\n    regex: (.+)\n  -
    source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]\n
    \   action: replace\n    regex: ([^:]+)(?::\\d+)?;(\\d+)\n    replacement: $1:$2\n
    \   target_label: __address__\n  - source_labels: [__meta_kubernetes_namespace]\n
    \   action: replace\n    target_label: namespace\n  - source_labels: [__meta_kubernetes_pod_name]\n
    \   action: replace\n    target_label: pod\n  - source_labels: [__meta_kubernetes_pod_label_app]\n
    \   action: replace\n    target_label: job\n\n  # drop very expensive apiserver
    metrics\n  metric_relabel_configs:\n  - source_labels: [__name__]\n    regex:
    'apiserver_request_(duration|latencies)_.*'\n    action: drop\n  - source_labels:
    [__name__]\n    regex: 'apiserver_response_sizes_.*'\n    action: drop\n\n# scrape
    other cluster control plane components, like kube-state-metrics, DNS resolver,\n#
    machine-controller etcd.\n- job_name: control-plane-pods\n  kubernetes_sd_configs:\n
    \ - role: pod\n    namespaces:\n      names:\n      - \"cluster-de-test-01\"\n\n
    \ relabel_configs:\n  - source_labels: [__meta_kubernetes_pod_label_app, __meta_kubernetes_pod_container_init]\n
    \   regex: \"kube-state-metrics;true\"\n    action: drop\n  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\n
    \   action: keep\n    regex: true\n  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\n
    \   action: replace\n    target_label: __metrics_path__\n    regex: (.+)\n  -
    source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]\n
    \   action: replace\n    regex: ([^:]+)(?::\\d+)?;(\\d+)\n    replacement: $1:$2\n
    \   target_label: __address__\n  - source_labels: [__meta_kubernetes_pod_label_role,
    __meta_kubernetes_pod_label_app]\n    action: replace\n    target_label: job\n
    \   separator: ''\n  - source_labels: [__meta_kubernetes_namespace]\n    action:
    replace\n    target_label: namespace\n  - source_labels: [__meta_kubernetes_pod_name]\n
    \   action: replace\n    target_label: pod\n\n#######################################################################\n#
    These rules will scrape pods running inside the user cluster itself.\n\n# scrape
    node metrics\n- job_name: nodes\n  scheme: https\n  tls_config:\n    ca_file:
    /etc/kubernetes/ca.crt\n    cert_file: /etc/kubernetes/prometheus-client.crt\n
    \   key_file: /etc/kubernetes/prometheus-client.key\n\n  kubernetes_sd_configs:\n
    \ - role: node\n    api_server: 'https://apiserver-external.cluster-de-test-01.svc.cluster.local.'\n
    \   tls_config:\n      ca_file: /etc/kubernetes/ca.crt\n      cert_file: /etc/kubernetes/prometheus-client.crt\n
    \     key_file: /etc/kubernetes/prometheus-client.key\n\n  relabel_configs:\n
    \ - action: labelmap\n    regex: __meta_kubernetes_node_label_(.+)\n  - target_label:
    __address__\n    replacement: 'apiserver-external.cluster-de-test-01.svc.cluster.local.'\n
    \ - source_labels: [__meta_kubernetes_node_name]\n    regex: (.+)\n    target_label:
    __metrics_path__\n    replacement: /api/v1/nodes/${1}/proxy/metrics\n\n# scrape
    node cadvisor\n- job_name: cadvisor\n  scheme: https\n  tls_config:\n    ca_file:
    /etc/kubernetes/ca.crt\n    cert_file: /etc/kubernetes/prometheus-client.crt\n
    \   key_file: /etc/kubernetes/prometheus-client.key\n\n  kubernetes_sd_configs:\n
    \ - role: node\n    api_server: 'https://apiserver-external.cluster-de-test-01.svc.cluster.local.'\n
    \   tls_config:\n      ca_file: /etc/kubernetes/ca.crt\n      cert_file: /etc/kubernetes/prometheus-client.crt\n
    \     key_file: /etc/kubernetes/prometheus-client.key\n\n  relabel_configs:\n
    \ - action: labelmap\n    regex: __meta_kubernetes_node_label_(.+)\n  - target_label:
    __address__\n    replacement: 'apiserver-external.cluster-de-test-01.svc.cluster.local.'\n
    \ - source_labels: [__meta_kubernetes_node_name]\n    regex: (.+)\n    target_label:
    __metrics_path__\n    replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor\n\n#
    scrape pods inside the user cluster with a special annotation\n- job_name: 'user-cluster-pods'\n
    \ scheme: https\n  tls_config:\n    ca_file: /etc/kubernetes/ca.crt\n    cert_file:
    /etc/kubernetes/prometheus-client.crt\n    key_file: /etc/kubernetes/prometheus-client.key\n\n
    \ kubernetes_sd_configs:\n  - role: pod\n    api_server: 'https://apiserver-external.cluster-de-test-01.svc.cluster.local.'\n
    \   tls_config:\n      ca_file: /etc/kubernetes/ca.crt\n      cert_file: /etc/kubernetes/prometheus-client.crt\n
    \     key_file: /etc/kubernetes/prometheus-client.key\n\n  relabel_configs:\n
    \ - source_labels: [__meta_kubernetes_pod_annotation_monitoring_kubermatic_io_port]\n
    \   action: keep\n    regex: \\d+\n  - source_labels: [__meta_kubernetes_pod_annotation_monitoring_kubermatic_io_path]\n
    \   regex: (.+)\n    action: replace\n    target_label: __metrics_path__\n  -
    source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_pod_name, __meta_kubernetes_pod_annotation_monitoring_kubermatic_io_port,
    __metrics_path__]\n    action: replace\n    regex: (.*);(.*);(.*);(.*)\n    target_label:
    __metrics_path__\n    replacement: /api/v1/namespaces/${1}/pods/${2}:${3}/proxy${4}\n
    \ - target_label: __address__\n    replacement: 'apiserver-external.cluster-de-test-01.svc.cluster.local.'\n
    \ - source_labels: [__meta_kubernetes_namespace]\n    action: replace\n    target_label:
    namespace\n  - source_labels: [__meta_kubernetes_pod_name]\n    action: replace\n
    \   target_label: pod\n\n# scrape kubelet resources\n- job_name: kubelet\n  scheme:
    https\n  tls_config:\n    ca_file: /etc/kubernetes/ca.crt\n    cert_file: /etc/kubernetes/prometheus-client.crt\n
    \   key_file: /etc/kubernetes/prometheus-client.key\n\n  kubernetes_sd_configs:\n
    \ - role: node\n    api_server: 'https://apiserver-external.cluster-de-test-01.svc.cluster.local.'\n\ttls_config:\n
    \     ca_file: /etc/kubernetes/ca.crt\n      cert_file: /etc/kubernetes/prometheus-client.crt\n
    \     key_file: /etc/kubernetes/prometheus-client.key\n\n  relabel_configs:\n
    \ - action: labelmap\n\tregex: __meta_kubernetes_node_label_(.+)\n  - target_label:
    __address__\n    replacement: 'apiserver-external.cluster-de-test-01.svc.cluster.local.'\n
    \ - source_labels: [__meta_kubernetes_node_name]\n\tregex: (.+)\n\ttarget_label:
    __metrics_path__\n\treplacement: /api/v1/nodes/${1}/proxy/metrics\n\n- job_name:
    resources\n  scheme: https\n  tls_config:\n    ca_file: /etc/kubernetes/ca.crt\n
    \   cert_file: /etc/kubernetes/prometheus-client.crt\n    key_file: /etc/kubernetes/prometheus-client.key\n\n
    \ kubernetes_sd_configs:\n  - role: node\n    api_server: 'https://apiserver-external.cluster-de-test-01.svc.cluster.local.'\n
    \   tls_config:\n      ca_file: /etc/kubernetes/ca.crt\n      cert_file: /etc/kubernetes/prometheus-client.crt\n
    \     key_file: /etc/kubernetes/prometheus-client.key\n\n  relabel_configs:\n
    \ - action: labelmap\n    regex: __meta_kubernetes_node_label_(.+)\n  - target_label:
    __address__\n    replacement: 'apiserver-external.cluster-de-test-01.svc.cluster.local.'\n
    \ - source_labels: [__meta_kubernetes_node_name]\n    regex: (.+)\n    target_label:
    __metrics_path__\n    replacement: /api/v1/nodes/${1}/proxy/metrics/resource\n\n#######################################################################\n#
    custom scraping configurations\n\n- job_name: custom-test-config\n  scheme: https\n
    \ metrics_path: '/metrics'\n  static_configs:\n  - targets:\n    - 'foo.bar:12345'\n"
  rules.yaml: |
    groups:
    - name: kubermatic.goprocess
      rules:
      - record: job:process_resident_memory_bytes:clone
        expr: process_resident_memory_bytes
        labels:
          kubermatic: federate

      - record: job:process_cpu_seconds_total:rate5m
        expr: rate(process_cpu_seconds_total[5m])
        labels:
          kubermatic: federate

      - record: job:process_open_fds:clone
        expr: process_open_fds
        labels:
          kubermatic: federate

    - name: kubermatic.etcd
      rules:
      - alert: EtcdInsufficientMembers
        annotations:
          message: 'Etcd cluster "{{ $labels.job }}": insufficient members ({{ $value }}).'
        expr: |
          sum(up{job="etcd"} == bool 1) by (job) < ((count(up{job="etcd"}) by (job) + 1) / 2)
        for: 15m
        labels:
          severity: critical

      - alert: EtcdNoLeader
        annotations:
          message: 'Etcd cluster "{{ $labels.job }}": member {{ $labels.instance }} has no leader.'
        expr: |
          etcd_server_has_leader{job="etcd"} == 0
        for: 15m
        labels:
          severity: critical

      - alert: EtcdHighNumberOfLeaderChanges
        annotations:
          message: 'Etcd cluster "{{ $labels.job }}": instance {{ $labels.instance }} has seen {{ $value }} leader changes within the last hour.'
        expr: |
          rate(etcd_server_leader_changes_seen_total{job="etcd"}[15m]) > 3
        for: 15m
        labels:
          severity: warning

      - alert: EtcdGRPCRequestsSlow
        annotations:
          message: 'Etcd cluster "{{ $labels.job }}": gRPC requests to {{ $labels.grpc_method }} are taking {{ $value }}s on etcd instance {{ $labels.instance }}.'
        expr: |
          histogram_quantile(0.99, sum(rate(grpc_server_handling_seconds_bucket{job="etcd", grpc_type="unary"}[5m])) by (job, instance, grpc_service, grpc_method, le))
          > 0.15
        for: 10m
        labels:
          severity: critical

      - alert: EtcdMemberCommunicationSlow
        annotations:
          message: 'Etcd cluster "{{ $labels.job }}": member communication with {{ $labels.To }} is taking {{ $value }}s on etcd instance {{ $labels.instance }}.'
        expr: |
          histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket{job="etcd"}[5m]))
          > 0.15
        for: 10m
        labels:
          severity: warning

      - alert: EtcdHighNumberOfFailedProposals
        annotations:
          message: 'Etcd cluster "{{ $labels.job }}": {{ $value }} proposal failures within the last hour on etcd instance {{ $labels.instance }}.'
        expr: |
          rate(etcd_server_proposals_failed_total{job="etcd"}[15m]) > 5
        for: 15m
        labels:
          severity: warning

      - alert: EtcdHighFsyncDurations
        annotations:
          message: 'Etcd cluster "{{ $labels.job }}": 99th percentile fync durations are {{ $value }}s on etcd instance {{ $labels.instance }}.'
        expr: |
          histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket{job="etcd"}[5m]))
          > 0.5
        for: 10m
        labels:
          severity: warning

      - alert: EtcdHighCommitDurations
        annotations:
          message: 'Etcd cluster "{{ $labels.job }}": 99th percentile commit durations {{ $value }}s on etcd instance {{ $labels.instance }}.'
        expr: |
          histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket{job="etcd"}[5m]))
          > 0.25
        for: 10m
        labels:
          severity: warning

      - alert: EtcdDatabaseQuotaLowSpace
        annotations:
          message: 'Etcd cluster "{{ $labels.job }}": database size exceeds the defined quota on etcd instance {{ $labels.instance }}, please defrag or increase the quota as the writes to etcd will be disabled when it is full.'
        expr: |
          (last_over_time(etcd_mvcc_db_total_size_in_bytes[5m]) / last_over_time(etcd_server_quota_backend_bytes[5m]))*100 > 95
        for: 10m
        labels:
          severity: critical

      - alert: EtcdExcessiveDatabaseGrowth
        annotations:
          message: 'Etcd cluster "{{ $labels.job }}": Predicting running out of disk space in the next four hours, based on write observations within the past four hours on etcd instance {{ $labels.instance }}, please check as it might be disruptive.'
        expr: |
          predict_linear(etcd_mvcc_db_total_size_in_bytes[4h], 4*60*60) > etcd_server_quota_backend_bytes
        for: 10m
        labels:
          severity: warning

      - alert: EtcdDatabaseHighFragmentationRatio
        annotations:
          message: 'Etcd cluster "{{ $labels.job }}": database size in use on instance {{ $labels.instance }} is {{ $value | humanizePercentage }} of the actual allocated disk space, please run defragmentation (e.g. etcdctl defrag) to retrieve the unused fragmented disk space.'
        expr: |
          (last_over_time(etcd_mvcc_db_total_size_in_use_in_bytes[5m]) / last_over_time(etcd_mvcc_db_total_size_in_bytes[5m])) < 0.5 and etcd_mvcc_db_total_size_in_use_in_bytes > 104857600
        for: 10m
        labels:
          severity: warning

      - record: job:etcd_server_has_leader:sum
        expr: sum(etcd_server_has_leader)
        labels:
          kubermatic: federate

      - record: job:etcd_disk_wal_fsync_duration_seconds_bucket:99percentile
        expr: histogram_quantile(0.99, sum(rate(etcd_disk_wal_fsync_duration_seconds_bucket[5m])) by (instance, le))
        labels:
          kubermatic: federate

      - record: job:etcd_disk_backend_commit_duration_seconds_bucket:99percentile
        expr: histogram_quantile(0.99, sum(rate(etcd_disk_backend_commit_duration_seconds_bucket[5m])) by (instance, le))
        labels:
          kubermatic: federate

      - record: job:etcd_mvcc_db_total_size_in_bytes:clone
        expr: etcd_mvcc_db_total_size_in_bytes
        labels:
          kubermatic: federate

      - record: job:etcd_network_client_grpc_received_bytes_total:rate5m
        expr: rate(etcd_network_client_grpc_received_bytes_total[5m])
        labels:
          kubermatic: federate

      - record: job:etcd_network_client_grpc_sent_bytes_total:rate5m
        expr: rate(etcd_network_client_grpc_sent_bytes_total[5m])
        labels:
          kubermatic: federate

      - record: job:etcd_network_peer_received_bytes_total:rate5msum
        expr: sum(rate(etcd_network_peer_received_bytes_total[5m])) by (instance)
        labels:
          kubermatic: federate

      - record: job:etcd_network_peer_sent_bytes_total:rate5msum
        expr: sum(rate(etcd_network_peer_sent_bytes_total[5m])) by (instance)
        labels:
          kubermatic: federate

      - record: job:etcd_server_proposals_failed_total:rate5msum
        expr: sum(rate(etcd_server_proposals_failed_total[5m]))
        labels:
          kubermatic: federate

      - record: job:etcd_server_proposals_pending:sum
        expr: sum(etcd_server_proposals_pending)
        labels:
          kubermatic: federate

      - record: job:etcd_server_proposals_committed_total:rate5msum
        expr: sum(rate(etcd_server_proposals_committed_total[5m]))
        labels:
          kubermatic: federate

      - record: job:etcd_server_proposals_applied_total:rate5msum
        expr: sum(rate(etcd_server_proposals_applied_total[5m]))
        labels:
          kubermatic: federate

      - record: job:etcd_server_leader_changes_seen_total:changes1d
        expr: changes(etcd_server_leader_changes_seen_total[1d])
        labels:
          kubermatic: federate

      - record: job:etcd_mvcc_delete_total:rate5m
        expr: rate(etcd_mvcc_delete_total[5m])
        labels:
          kubermatic: federate

      - record: job:etcd_mvcc_put_total:rate5m
        expr: rate(etcd_mvcc_put_total[5m])
        labels:
          kubermatic: federate

      - record: job:etcd_mvcc_range_total:rate5m
        expr: rate(etcd_mvcc_range_total[5m])
        labels:
          kubermatic: federate

      - record: job:etcd_debugging_mvcc_watcher_total:rate5m
        expr: rate(etcd_debugging_mvcc_watcher_total[5m])
        labels:
          kubermatic: federate

      - record: job:etcd_mvcc_txn_total:rate5m
        expr: rate(etcd_mvcc_txn_total[5m])
        labels:
          kubermatic: federate

      - record: job:etcd_debugging_mvcc_keys_total:clone
        expr: etcd_debugging_mvcc_keys_total
        labels:
          kubermatic: federate

      - record: job:etcd_debugging_store_reads_total:rate5m
        expr: rate(etcd_debugging_store_reads_total[5m])
        labels:
          kubermatic: federate

      - record: job:etcd_debugging_store_writes_total:rate5m
        expr: rate(etcd_debugging_store_writes_total[5m])
        labels:
          kubermatic: federate

      - record: job:etcd_debugging_store_expires_total:rate5m
        expr: rate(etcd_debugging_store_expires_total[5m])
        labels:
          kubermatic: federate

    - name: machine-controller
      rules:
      - alert: MachineControllerTooManyErrors
        annotations:
          message: Machine Controller in {{ $labels.namespace }} has too many errors in its loop.
        expr: |
          sum(rate(machine_controller_errors_total[5m])) by (namespace) > 0.01
        for: 20m
        labels:
          severity: warning

      - alert: MachineControllerMachineDeletionTakesTooLong
        annotations:
          message: Machine {{ $labels.machine }} of cluster {{ $labels.cluster }} is stuck in deletion for more than 30min.
        expr: (time() - machine_controller_machine_deleted) > 30*60
        for: 0m
        labels:
          severity: warning

      - alert: AWSInstanceCountTooHigh
        annotations:
          message: '{{ $labels.machine }} has more than one instance at AWS'
        expr: machine_controller_aws_instances_for_machine > 1
        for: 30m
        labels:
          severity: warning

      - alert: KubernetesAdmissionWebhookHighRejectionRate
        annotations:
          message: '{{ $labels.operation }} requests for Machine objects are failing (Admission) with a high rate. Consider checking the affected objects'
        expr: rate(apiserver_admission_webhook_admission_latencies_seconds_count{name="machine-controller.kubermatic.io-machines",rejected="true"}[5m]) > 0.01
        for: 5m
        labels:
          severity: warning

      - record: job:machine_controller_errors_total:rate5m
        expr: rate(machine_controller_errors_total[5m])
        labels:
          kubermatic: federate

      - record: job:machine_controller_workers:sum
        expr: sum(machine_controller_workers)
        labels:
          kubermatic: federate

      - record: job:machine_controller_machines_total:sum
        expr: sum(machine_controller_machines_total) by (kubelet_version, os, provider, size)
        labels:
          kubermatic: federate

    - name: process.filedescriptors
      rules:
      - expr: process_open_fds / process_max_fds
        record: instance:fd_utilization

      - alert: FdExhaustionClose
        annotations:
          message: '{{ $labels.job }} instance {{ $labels.instance }} will exhaust its file descriptors soon'
        expr: |
          predict_linear(instance:fd_utilization[1h], 3600 * 4) > 1
        for: 10m
        labels:
          severity: warning

      - alert: FdExhaustionClose
        annotations:
          message: '{{ $labels.job }} instance {{ $labels.instance }} will exhaust its file descriptors soon'
        expr: |
          predict_linear(instance:fd_utilization[10m], 3600) > 1
        for: 10m
        labels:
          severity: critical

    - name: kubernetes-nodes
      rules:
      - alert: KubernetesNodeNotReady
        annotations:
          message: '{{ $labels.node }} has been unready for more than an hour.'
        expr: kube_node_status_condition{condition="Ready",status="true"} == 0
        for: 30m
        labels:
          severity: warning

      - record: job:kube_node_info:count
        expr: count(kube_node_info)
        labels:
          kubermatic: federate

    - name: kubernetes-absent
      rules:
      - alert: KubernetesApiserverDown
        annotations:
          message: Kubernetes apiserver has disappeared from Prometheus target discovery.
        expr: absent(up{job="apiserver"} == 1)
        for: 15m
        labels:
          severity: critical

      - alert: MachineControllerDown
        annotations:
          message: Machine controller has disappeared from Prometheus target discovery.
        expr: absent(up{job="machine-controller"} == 1)
        for: 15m
        labels:
          severity: critical

      - alert: UserClusterControllerDown
        annotations:
          message: User Cluster Controller has disappeared from Prometheus target discovery.
        expr: absent(up{job="usercluster-controller"} == 1)
        for: 15m
        labels:
          severity: critical

      - alert: KubeStateMetricsDown
        annotations:
          message: Kube-state-metrics has disappeared from Prometheus target discovery.
        expr: absent(up{job="kube-state-metrics"} == 1)
        for: 15m
        labels:
          severity: warning

      - alert: EtcdDown
        annotations:
          message: Etcd has disappeared from Prometheus target discovery.
        expr: absent(up{job="etcd"} == 1)
        for: 15m
        labels:
          severity: critical

      # This is triggered if the cluster does have nodes, but the cadvisor could
      # not successfully be scraped for whatever reason. An absent() on cadvisor
      # metrics is not a good alert because clusters could simply have no nodes
      # and hence no cadvisors.
      - alert: CAdvisorDown
        annotations:
          message: cAdvisor on {{ $labels.kubernetes_io_hostname }} could not be scraped.
        expr: up{job="cadvisor"} == 0
        for: 15m
        labels:
          severity: warning

      # This functions similarly to the cadvisor alert above.
      - alert: KubernetesNodeDown
        annotations:
          message: The kubelet on {{ $labels.kubernetes_io_hostname }} could not be scraped.
        expr: up{job="kubernetes-nodes"} == 0
        for: 15m
        labels:
          severity: warning
metadata:
  creationTimestamp: null
  labels:
    app: prometheus
  name: prometheus
  namespace: cluster-de-test-01
