# Copyright 2022 The Kubermatic Kubernetes Platform contributors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

cortex:
  # Check master values.yaml which are the default values for Chart version v3.0.0
  # https://github.com/cortexproject/cortex-helm-chart/blob/v3.0.0/values.yaml

  # image:
  #   tag: "v1.20.0"
  config:
    auth_enabled: true
    # Note: below defaults were changed by cortex from 0.7.0 to 1.7.0. So not keeping old defaults
    # ingester:
    #  lifecycler:
    #     join_after: 0s
    #     final_sleep: 0s

    #     Note: consul is removed from the setup as per below recommendation.
    #     Recommendation to remove consul: https://github.com/cortexproject/cortex-helm-chart/blob/v2.1.0/README.md#key-value-store
    #     ring:
    #       kvstore:
    #         consul:
    #           host: 'consul:8500'
    #           http_client_timeout: '20s'
    #           consistent_reads: true
    limits:
      # Note: below defaults were changed by cortex from 0.7.0 to 1.7.0. So not keeping old defaults in upgrade to helm chart v2.1.0
      # enforce_metric_name: false

      # Note: max_query_lookback is limiting query only till 7 days. This is not needed. We should control storage via retention_period.
      # Query should be allowed to go back until last of storage.
      # ref: https://github.com/kubermatic/mla/commit/b49e93289fe013452ba4b4134f9da4ef7ef88df1
      # max_query_lookback: 168h

      # Note: below were added for HA setup and stability of the cluster in kubermatic setup. So should be kept.
      accept_ha_samples: true
      max_label_names_per_series: 40
      # avoid "err: out of order sample." errors in both agent and cortex side.
      out_of_order_time_window: 10m
    # Note: below defaults were changed by cortex from 0.7.0 to 1.7.0. So not keeping old defaults in upgrade to helm chart v2.1.0
    # server:
    #   grpc_server_max_concurrent_streams: 1000
    distributor:
      # Note: Retaining below customization to enable accepting ha prometheus metrics
      ha_tracker:
        enable_ha_tracker: true
        # consul is required as ha_tracker does not support memberlist
        kvstore:
          store: consul
          consul:
            host: consul-consul-server:8500
      ring:
        kvstore:
          store: memberlist
    memberlist:
      # TODO: review if we customized below or are defaults from helmchart v1.7.0
      # This needs to be re-reviewed. There is now a new `memberlist` tag at top level in values.yaml and so may be we can remove the customization.
      join_members:
        - cortex-ingester-headless
      #  - '{{ include "cortex.fullname" $ }}-memberlist'
    querier:
      # Note: below defaults were changed by cortex from 0.7.0 to 1.7.0. So not keeping old defaults in upgrade to helm chart v2.1.0
      # active_query_tracker_dir: /data/cortex/querier

      # Note: below changes were done specifically in kubermatic config so they are retained.
      # Ref: https://github.com/kubermatic/mla/commit/b49e93289fe013452ba4b4134f9da4ef7ef88df1
      query_ingesters_within: 365m
      query_store_after: 360m
    ruler:
      # This is specifically customized for kubermatic setup so keeping it as is.
      enable_alertmanager_discovery: true
      # Note: This is a KKP-specific headless alertmanager service, used to work around the Cortex helm chart bug in chart versions below v1.0.0
      # alertmanager_url: http://_http-metrics._tcp.cortex-alertmanager-headless-kkp/api/prom/alertmanager/
      # Note: below setting is kept as we don't want to turn on consul for this ring
      ring:
        kvstore:
          store: memberlist
    ruler_storage:
      s3:
        bucket_name: "cortex-ruler"
        endpoint: "minio:9000"
        insecure: true
    # runtime_config uses defaults from upstream v3.0.0:
    #   file: /etc/cortex-runtime-config/runtime_config.yaml
    #   period: 10s
    alertmanager:
      # Note: These are customized in kubermatic. Not sure if they are really needed. But will keep them.
      enable_api: true
      data_dir: /data/cortex/alert-data
    alertmanager_storage:
      s3:
        bucket_name: "alertmanager"
        endpoint: "minio:9000"
        insecure: true
    compactor:
      # improve data fetch from s3. This increases parallelism while fetching blocks from object storage
      blocks_fetch_concurrency: 10
      data_dir: /data/cortex/compactor
      sharding_enabled: true
      sharding_ring:
        kvstore:
          store: memberlist
    blocks_storage:
      bucket_store:
        sync_dir: /data
        ignore_deletion_mark_delay: 1h
      s3:
        bucket_name: "cortex"
        endpoint: "minio:9000"
        insecure: true
      tsdb:
        dir: /data
        retention_period: 365m
        close_idle_tsdb_timeout: 365m
        wal_compression_enabled: true
        flush_blocks_on_shutdown: true
  runtimeconfigmap:
    # -- If true, a configmap for the `runtime_config` will be created.
    # If false, the configmap _must_ exist already on the cluster or pods will fail to create.
    # TODO: review if we customized below or are defaults from helmchart v1.7.0
    create: false
  alertmanager:
    replicas: 2
    statefulSet:
      enabled: true
    extraArgs:
      alertmanager-storage.s3.access-key-id: $(ACCESS_KEY)
      alertmanager-storage.s3.secret-access-key: $(SECRET_KEY)
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                  - key: target
                    operator: In
                    values:
                      - alertmanager
              topologyKey: "kubernetes.io/hostname"
    persistentVolume:
      storageClass: "kubermatic-fast"
    extraVolumeMounts:
      - mountPath: /tmp
        name: storage
    env:
      - name: ACCESS_KEY
        valueFrom:
          secretKeyRef:
            name: minio
            key: rootUser
      - name: SECRET_KEY
        valueFrom:
          secretKeyRef:
            name: minio
            key: rootPassword
  distributor:
    podAnnotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict-local-volumes: storage
  ingester:
    statefulSet:
      enabled: true
    extraArgs:
      blocks-storage.s3.access-key-id: $(ACCESS_KEY)
      blocks-storage.s3.secret-access-key: $(SECRET_KEY)
    persistentVolume:
      # TODO: review if we customized below or are defaults from helmchart v1.7.0
      size: 10Gi
      storageClass: "kubermatic-fast"

    # As per latest values.yaml - Startup/liveness probes for ingesters are not recommended.
    # -- Startup/liveness probes for ingesters are not recommended.
    #  Ref: https://cortexmetrics.io/docs/guides/running-cortex-on-kubernetes/#take-extra-care-with-ingesters
    # startupProbe:
    #   # WAL Replay can take a long time. Increasing failureThreshold for ~30 min of time until killed
    #   failureThreshold: 60
    #   initialDelaySeconds: 120
    #   periodSeconds: 30
    #   httpGet:
    #     path: /ready
    #     port: http-metrics
    #     scheme: HTTP
    # livenessProbe:
    #   httpGet:
    #     path: /ready
    #     port: http-metrics
    #     scheme: HTTP
    env:
      - name: ACCESS_KEY
        valueFrom:
          secretKeyRef:
            name: minio
            key: rootUser
      - name: SECRET_KEY
        valueFrom:
          secretKeyRef:
            name: minio
            key: rootPassword
  ruler:
    # Note: this is specifically customized in kubermatic. So keeping it that way.
    resources:
      requests:
        cpu: 5m
    extraArgs:
      ruler-storage.s3.access-key-id: $(ACCESS_KEY)
      ruler-storage.s3.secret-access-key: $(SECRET_KEY)
    podAnnotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict-local-volumes: storage,tmp
    env:
      - name: ACCESS_KEY
        valueFrom:
          secretKeyRef:
            name: minio
            key: rootUser
      - name: SECRET_KEY
        valueFrom:
          secretKeyRef:
            name: minio
            key: rootPassword
  querier:
    # Note: this is specifically customized in kubermatic. So keeping it that way.
    resources:
      requests:
        cpu: 5m
    extraArgs:
      blocks-storage.s3.access-key-id: $(ACCESS_KEY)
      blocks-storage.s3.secret-access-key: $(SECRET_KEY)
    podAnnotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict-local-volumes: storage
    env:
      - name: ACCESS_KEY
        valueFrom:
          secretKeyRef:
            name: minio
            key: rootUser
      - name: SECRET_KEY
        valueFrom:
          secretKeyRef:
            name: minio
            key: rootPassword
  query_frontend:
    # Note: this is specifically customized in kubermatic. So keeping it that way.
    replicas: 1
    resources:
      requests:
        cpu: 5m
  nginx:
    # Note: this is configured specifically in kubermatic so keeping it that way. I am yet to understand why we need nginx.
    enabled: false
  store_gateway:
    resources:
      # Note: this is a change is done in Kubermatic side so retaining.
      requests:
        cpu: 5m
    extraArgs:
      blocks-storage.s3.access-key-id: $(ACCESS_KEY)
      blocks-storage.s3.secret-access-key: $(SECRET_KEY)
    persistentVolume:
      storageClass: "kubermatic-fast"
    env:
      - name: ACCESS_KEY
        valueFrom:
          secretKeyRef:
            name: minio
            key: rootUser
      - name: SECRET_KEY
        valueFrom:
          secretKeyRef:
            name: minio
            key: rootPassword
  compactor:
    persistentVolume:
      storageClass: "kubermatic-fast"
  memcached-blocks-index:
    enabled: true
    podAnnotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict-local-volumes: tmp
    # Note: this is specifically customized in kubermatic. So keeping it that way.
    # Note: Helm chart v3.0.0 uses community memcached image, config via args instead of extraEnvVars
    resources:
      requests:
        cpu: 5m
  memcached-blocks:
    enabled: true
    podAnnotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict-local-volumes: tmp
    # Note: this is specifically customized in kubermatic. So keeping it that way.
    resources:
      requests:
        cpu: 5m
  memcached-blocks-metadata:
    enabled: true
    podAnnotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict-local-volumes: tmp
    # Note: this is specifically customized in kubermatic. So keeping it that way.
    resources:
      requests:
        cpu: 5m

