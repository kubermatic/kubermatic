# Copyright 2020 The Kubermatic Kubernetes Platform contributors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

prometheus:
  replicas: 2
  image:
    repository: quay.io/prometheus/prometheus
    tag: v2.51.1
    pullPolicy: IfNotPresent
  # list of image pull secret references, e.g.
  # imagePullSecrets:
  #   - name: quay-io-pull-secret
  imagePullSecrets: []

  host: ''
  storageSize: 100Gi
  storageClass: kubermatic-fast

  tsdb:
    retentionTime: 15d
    compressWAL: true

  configReloaderImage:
    repository: ghcr.io/jimmidyson/configmap-reload
    tag: v0.12.0
    pullPolicy: IfNotPresent

  # If you install Prometheus using a different Helm release name,
  # you can override the name used for the resources, e.g. to have
  # multiple Prometheus installed into distinct namespaces but still
  # have the same Service names, ConfigMap names etc.
  #nameOverride: prometheus

  backup:
    enabled: true
    image:
      repository: quay.io/kubermatic/util
      tag: 2.5.0
    timeout: 60m

  # Specify additional external labels which will be added to all
  # alerts sent by Prometheus.
  #externalLabels:
  #  seed_cluster: default

  # Configure the scraping rules for Prometheus. You can either
  # add your own scraping configs here or change the path to the
  # predefined config files that are evaluated when Helm builds
  # the chart and deploys it. You cannot use this to load files
  # at runtime from a custom volume because Prometheus does not
  # support it.
  scraping:
    files:
    - config/scraping/*.yaml

    #configs:
    #- job_name: myscrapejob
    #  honor_labels: true
    #  ...

    # Enable Blackbox Exporter scraping rules, this requires to deploy blackbox exporter in the cluster.
    blackBoxExporter:
      enabled: false
      url: blackbox-exporter:9115
    # Enable if minio is running with tls in the same cluster
    minio:
      tls:
        enabled: false
      # Change values if minio running in different namespace and having different label. default is is set to minio.
      #namespace: minio
      #appLabel: minio
    # Master/seed etcd metrics scraping can be enabled by providing the etcd client cert & key, this is required for prometheus to scrape metrics using TLS.
    etcd:
      tls:
        crt: ''
        key: ''
  # Similarly to the scraping config, you can configure the
  # target alertmanagers here.
  alerting:
    alert_relabel_configs:
    - action: labeldrop
      regex: replica
    
  alertmanagers:
    files:
    - config/alertmanagers/*.yaml
    #configs:
    #- scheme: http
    #  path_prefix: /
    #  ...

  # The list of rule files to load; if you use the `volumes`
  # directive below to mount your own ConfigMap or Secret into
  # Prometheus, you will want to extend this list to load your
  # own rule files. You can remove the predefined path to
  # effectively disable the stock recordings and alerts.
  ruleFiles:
  - /etc/prometheus/rules/general-*.yaml
  - /etc/prometheus/rules/kubermatic-seed-*.yaml
  # If you are running a non-master cluster, you should comment the
  # following line to disable master components alerts.
  - /etc/prometheus/rules/kubermatic-master-*.yaml
  # If you run in an environment where access to Kubernetes
  # scheduler and controller-manager is not possible (like GKE),
  # disable the expression below to not create false alerts
  # for missing/unhealthy components.
  - /etc/prometheus/rules/managed-*.yaml

  # If user cluster MLA is enabled, uncomment the following line
  # to enable alerting for cortex and loki.
  # - /etc/prometheus/rules/usercluster-mla-*.yaml

  # Optionally add some more recording/alerting rules; the structure
  # beneath `rules` is identical to regular rules files as documented
  # in https://prometheus.io/docs/prometheus/latest/getting_started/
  # For larger collections of rules, consider using the custom volume
  # approach shown further down in the `volumes` section.
  #rules:
  #  groups:
  #    - name: myrules
  #      rules:
  #      - alert: DatacenterIsOnFire
  #        expr: temperature{cpu} > 100
  #        for: 5m

  # If you prefer to manage your recording/alerting rules in your
  # own ConfigMaps or Secrets, you can use this section to mount
  # those into the Prometheus pods. Remember to extend the `ruleFiles`
  # section above to have your files be loaded into Prometheus.
  # For each volume, specify either a configMap name or a secretName,
  # never both.
  #volumes:
  #- name: initech-alerting-rules
  #  mountPath: /initech/alerts
  #  configMap: initech-alerting-rules-configmap
  #- name: initech-recording-rules
  #  mountPath: /initech/recordings
  #  secretName: initech-recording-rules-secret

  # Optionally configure remote write from Prometheus instances to given targets.
  # https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write
  remoteWrite:
  #- url: http://host.example.com:12345/api/v1/receive

  ## Additional Prometheus server container arguments
  ##
  extraArgs: {}

  ## Add any sidecar containers e.g. thanos using this block
  # Key/Value where Key is the sidecar `- name: <Key>`
  # Example:
  #   sidecarContainers:
  #     - thanos:
  #         image: quay.io/thanos/thanos:v0.31.0
  sidecarContainers: []

  ## expose additional ports for sidecar containers e.g. when using thanos-sidecar
  # extraPorts:
  # - name: grpc
  #   port: 10901
  #   targetPort: grpc
  extraPorts: []

  containers:
    prometheus:
      resources:
        requests:
          cpu: 1
          memory: 3Gi
        limits:
          cpu: 2
          memory: 6Gi
    backup:
      resources:
        requests:
          cpu: 100m
          memory: 64Mi
        limits:
          cpu: 500m
          memory: 10Gi
    reloader:
      resources:
        requests:
          cpu: 5m
          memory: 24Mi
        limits:
          cpu: 5m
          memory: 32Mi

  nodeSelector: {}
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchLabels:
              app: '{{ template "name" . }}'
          topologyKey: kubernetes.io/hostname
        weight: 100
  tolerations: []
