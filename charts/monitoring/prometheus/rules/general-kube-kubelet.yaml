# This file has been generated, DO NOT EDIT.

# Copyright 2020 The Kubermatic Kubernetes Platform contributors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
groups:
  - name: kube-kubelet
    rules:
      - alert: KubeletDown
        annotations:
          message: Kubelet has disappeared from Prometheus target discovery.
          runbook_url: https://docs.kubermatic.com/kubermatic/master/cheat_sheets/alerting_runbook/#alert-kubeletdown
        expr: absent(up{job="kubelet"} == 1)
        for: 15m
        labels:
          severity: critical
          resource: '{{ $labels.instance }}'
          service: kubelet
      - alert: KubePersistentVolumeUsageCritical
        annotations:
          message: The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} is only {{ printf "%0.0f" $value }}% free.
          runbook_url: https://docs.kubermatic.com/kubermatic/master/cheat_sheets/alerting_runbook/#alert-kubepersistentvolumeusagecritical
        expr: |
          100 * kubelet_volume_stats_available_bytes{job="kubelet"}
            /
          kubelet_volume_stats_capacity_bytes{job="kubelet"}
            < 3
        for: 1m
        labels:
          severity: critical
          service: kubelet
          resource: '{{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }}'
      - alert: KubePersistentVolumeFullInFourDays
        annotations:
          message: Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} is expected to fill up within four days. Currently {{ $value }} bytes are available.
          runbook_url: https://docs.kubermatic.com/kubermatic/master/cheat_sheets/alerting_runbook/#alert-kubepersistentvolumefullinfourdays
        expr: |
          (
            kubelet_volume_stats_used_bytes{job="kubelet"}
              /
            kubelet_volume_stats_capacity_bytes{job="kubelet"}
          ) > 0.85
          and
          predict_linear(kubelet_volume_stats_available_bytes{job="kubelet"}[6h], 4 * 24 * 3600) < 0
        for: 5m
        labels:
          severity: critical
          service: kubelet
          resource: '{{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }}'
      - alert: KubeletTooManyPods
        annotations:
          message: Kubelet {{ $labels.instance }} is running {{ $value }} pods, close to the limit of 110.
          runbook_url: https://docs.kubermatic.com/kubermatic/master/cheat_sheets/alerting_runbook/#alert-kubelettoomanypods
        expr:  |
          count by(cluster, node) (
          (kube_pod_status_phase{job="kube-state-metrics",phase="Running"} == 1) * on(instance,pod,namespace,cluster) group_left(node) topk by(instance,pod,namespace,cluster) (1, kube_pod_info{job="kube-state-metrics"})
          )
          /
          max by(cluster, node) (
          kube_node_status_capacity{job="kube-state-metrics",resource="pods"} != 1
          ) > 0.95 
        for: 15m
        labels:
          severity: info
          resource: '{{ $labels.instance }}'
          service: kubelet
      - alert: KubeletClientErrors
        annotations:
          message: The kubelet on {{ $labels.instance }} is experiencing {{ printf "%0.0f" $value }}% errors.
          runbook_url: https://docs.kubermatic.com/kubermatic/master/cheat_sheets/alerting_runbook/#alert-kubeletclienterrors
        expr: |
          (sum(rate(rest_client_requests_total{code=~"(5..|<error>)",job="kubelet"}[5m])) by (instance)
            /
          sum(rate(rest_client_requests_total{job="kubelet"}[5m])) by (instance))
          * 100 > 1
        for: 15m
        labels:
          severity: warning
          resource: '{{ $labels.instance }}'
          service: kubelet
      # a dedicated rule for pods to include more helpful labels in the message like the instance and job name
      - alert: KubeClientErrors
        annotations:
          message: The pod {{ $labels.namespace }}/{{ $labels.pod }} is experiencing {{ printf "%0.0f" $value }}% errors.
          runbook_url: https://docs.kubermatic.com/kubermatic/master/cheat_sheets/alerting_runbook/#alert-kubeclienterrors
        expr: |
          (sum(rate(rest_client_requests_total{code=~"(5..|<error>)",job="pods"}[5m])) by (namespace, pod)
            /
          sum(rate(rest_client_requests_total{job="pods"}[5m])) by (namespace, pod))
          * 100 > 1
        for: 15m
        labels:
          severity: warning
          resource: '{{ $labels.instance }}'
          service: kubelet
      - alert: KubeletRuntimeErrors
        annotations:
          message: The kubelet on {{ $labels.instance }} is having an elevated error rate for container runtime oprations.
          runbook_url: https://docs.kubermatic.com/kubermatic/master/cheat_sheets/alerting_runbook/#alert-kubeletruntimeerrors
        expr: |
          sum(rate(kubelet_runtime_operations_errors_total{job="kubelet"}[5m])) by (instance) > 0.1
        for: 15m
        labels:
          severity: warning
          resource: '{{ $labels.instance }}'
          service: kubelet
      - alert: KubeletCGroupManagerDurationHigh
        annotations:
          message: The kubelet's cgroup manager duration on {{ $labels.instance }} has been elevated ({{ printf "%0.2f" $value }}ms) for more than 15 minutes.
          runbook_url: https://docs.kubermatic.com/kubermatic/master/cheat_sheets/alerting_runbook/#alert-kubeletcgroupmanagerlatencyhigh
        expr: |
          sum(rate(kubelet_cgroup_manager_duration_seconds{quantile="0.9"}[5m])) by (instance) * 1000 > 1
        for: 15m
        labels:
          resource: '{{ $labels.instance }}'
          service: kubelet
          severity: warning
      - alert: KubeletPodWorkerDurationHigh
        annotations:
          message: The kubelet's pod worker duration for {{ $labels.operation_type }} operations on {{ $labels.instance }} has been elevated ({{ printf "%0.2f" $value }}ms) for more than 15 minutes.
          runbook_url: https://docs.kubermatic.com/kubermatic/master/cheat_sheets/alerting_runbook/#alert-kubeletpodworkerdurationhigh
        expr: |
          sum(rate(kubelet_pod_worker_duration_seconds{quantile="0.9"}[5m])) by (instance, operation_type) * 1000 > 250
        for: 15m
        labels:
          severity: warning
          resource: '{{ $labels.instance }}/{{ $labels.operation_type }}'
          service: kubelet
      - alert: KubeVersionMismatch
        annotations:
          message: There are {{ $value }} different versions of Kubernetes components running.
          runbook_url: https://docs.kubermatic.com/kubermatic/master/cheat_sheets/alerting_runbook/#alert-kubeversionmismatch
        expr: count(count(kubernetes_build_info{job!="dns"}) by (gitVersion)) > 1
        for: 1h
        labels:
          severity: warning
      - alert: KubeContainerWaiting
        annotations:
          message: pod/{{ $labels.pod }} in namespace {{ $labels.namespace }} on container {{ $labels.container}} has been in waiting state for longer than 1 hour.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecontainerwaiting "
          summary: Pod container waiting longer than 1 hour
        expr: sum by (namespace, pod, container, cluster) (kube_pod_container_status_waiting_reason{job="kube-state-metrics"}) > 0
        for: 1h
        labels:
          severity: warning
		      resource: {{ $labels.pod }} in namespace {{ $labels.namespace }} on container {{ $labels.container}}
		      service: KubeContainer   
      - alert: KubeHpaReplicasMismatch
        annotations:
          message: HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }} has not matched the desired number of replicas for longer than 15 minutes.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubehpareplicasmismatch
          summary: HPA has not matched descired number of replicas.
        expr: | 
          (kube_horizontalpodautoscaler_status_desired_replicas{job="kube-state-metrics"}
          !=
          kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics"})
          and
          kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics"}
          >
          kube_horizontalpodautoscaler_spec_min_replicas{job="kube-state-metrics"})
          and
          (kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics"}
          <
          kube_horizontalpodautoscaler_spec_max_replicas{job="kube-state-metrics"})
          and
          changes(kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics"}[15m]) == 0
        for: 15m
        labels:
          severity: warning
		      resource: {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }}
		      service: KubeHpaReplicas
	  - alert: KubeHpaMaxedOut
      annotations:
        message: HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }} has been running at max replicas for longer than 15 minutes.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubehpamaxedout
        summary: HPA is running at max replicas
      expr: |
        kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics"}
        ==
        kube_horizontalpodautoscaler_spec_max_replicas{job="kube-state-metrics"}
      for: 15m
      labels:
        severity: warning
		    resource: {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }}
		    service: KubeHpa
    - alert: KubeletPlegDurationHigh
      annotations:
        message: The Kubelet Pod Lifecycle Event Generator has a 99th percentile duration of {{ $value }} seconds on node {{ $labels.node }}.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeletplegdurationhigh
        summary: Kubelet Pod Lifecycle Event Generator is taking too long to relist.
      expr: |
        node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile{quantile="0.99"} >= 10
      for: 5m
      labels:
        severity: warning
		    resource: {{ $value }} seconds on node {{ $labels.node }}
		    service: KubeletPlegDuration
    - alert: KubeletPodStartUpLatencyHigh
      annotations:
        message: Kubelet Pod startup 99th percentile latency is {{ $value }} seconds on node {{ $labels.node }}.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeletpodstartuplatencyhigh
        summary: Kubelet Pod startup latency is too high.
      expr: |
         histogram_quantile(0.99, sum(rate(kubelet_pod_worker_duration_seconds_bucket{job="kubelet"}[5m])) by (cluster, instance, le)) * on(cluster, instance) group_left(node) kubelet_node_name{job="kubelet"} > 60
      for: 15m
      labels:
        severity: warning
		    resource: {{ $value }} seconds on node {{ $labels.node }}
		    service: KubeletPodStartUpLatency