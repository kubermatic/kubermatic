# Scripts list and their description

This file is generated by running `hack/meta.sh`.

## changelog-gen.sh

Create a changelog since last release, commit and create a new release tag

    Usage:
    changelog-gen.sh -r v2.x.x - create changelog, commit and tag new release, using closed PRs release-note

## coverage.sh

Generate test coverage statistics for Go packages.

    Usage: coverage.sh [--html]

    --html      Additionally create HTML report and open it in browser

## fmt-grafana-dashboard.sh

TBD

## lib.sh

Contains commonly used functions for the other scripts.

## meta.sh

This README.md generator

That generates README.md with all scripts from this directory described.

## mirror-system-application-charts.sh

This script mirrors upstream Helm charts (used by System Applications) to the Kubermatic OCI registry.

#### Adding a New Chart

To add a new chart:

1. Add the chart's download URL template to the `CHART_URLS` array in the script:

   ```bash
   ["chart-name"]="https://example.com/chart-%s.tgz"
   ```

2. Add the default version for the chart to the `CHART_VERSIONS` array:

   ```bash
   ["chart-name"]="1.0.0"
   ```

#### Mirroring a Specific Version

To mirror a specific version (optional) of a chart, run:

```bash
./mirror-system-application-charts.sh <chart-name> [version (optional)]
```

For more details, refer to the script's comments.

## release-images.sh

Builds and pushes all KKP container images:

* quay.io/kubermatic/kubermatic[-ee]
* quay.io/kubermatic/addons
* quay.io/kubermatic/nodeport-proxy
* quay.io/kubermatic/kubeletdnat-controller
* quay.io/kubermatic/user-ssh-keys-agent
* quay.io/kubermatic/etcd-launcher
* quay.io/kubermatic/network-interface-manager

The images are tagged with all arguments given to the script, i.e
`./release-images.sh foo bar` will tag `kubermatic:foo` and
`kubermatic:bar`.

Before running this script, all binaries in `cmd/` must have been
built already by running `make build`.

## release-utility-images.sh

This script builds all the utility container images required by or provided
in KKP, like the http-prober, S3 exporter, util etc.
These utilities are not versioned together with KKP and so this script will
only build and push images if they are not yet present in the target registry.

## retag-images.sh

This script takes YAML on stdin and tries to find all references
to Docker images. The found images will then be pulled, retagged
with `$TARGET_REGISTRY` as their registry and pushed to said target
registry.

The purpose of this is to mirror all images used in a KKP setup
to prewarm a local Docker registry, for example in offline setups.

## run-conformance-tests.sh

Compiles the conformance tests and then runs them in a local Docker
container (by default). This requires KKP and an OIDC provider (like Dex)
to be installed, with a `$KUBECONFIG` pointing to the KKP master cluster.

The tests run against a single provider, specified via the `PROVIDER`
environment variable (default: `aws`). See this script for the
credential variables for each provider.

OIDC credentials need to be provided either by specifying
`KUBERMATIC_OIDC_LOGIN` and `KUBERMATIC_OIDC_PASSWORD` environment
variables or by setting `CREATE_OIDC_TOKEN=false` and setting
a predefined `KUBEMATIC_OIDC_TOKEN` variable.

Run this script with `-help` to see a list of all available flags on
the conformance tests. Many of these are set by this script, but you
can add and override as you like. NB: If test tests run inside a
container, make sure paths and environment variables can be properly
resolved.

To disable the Docker container, set the variable `NO_DOCKER=true`.
In this mode, you need to have the kube-test binaries and all other
dependencies installed locally on your machine, but it makes testing
against a local KKP setup much easier.

## run-expose-strategy-e2e-test-in-kind.sh

This script sets up a local KKP installation in kind, deploys a
couple of test Presets and Users and then runs the e2e tests for the
nodeport-proxy.

## run-integration-tests.sh

TBD

## run-machine-controller.sh

TBD

## run-master-controller-manager.sh

TBD

## run-nodeport-proxy-e2e-test-in-kind.sh

This script sets up a local KKP installation in kind, deploys a
couple of test Presets and Users and then runs the e2e tests for the
nodeport-proxy.

## run-operator.sh

TBD

## run-seed-controller-manager.sh

TBD

## run-tests.sh

TBD

## run-user-cluster-controller-manager.sh

TBD

## test-chart-rendering.sh

Used for "golden master" testing.
Symlink this script to a directory called "test" in the root directory of a chart.
Prepared test fixtures should end with .yaml, results will end with .yaml.out.
Script exits with 1 if the output of rendering is different than what is stored the .yaml.out file

## update-cert-manager-crds.sh

TBD

## update-codegen.sh

TBD

## update-docs.sh

TBD

## update-fixtures.sh

TBD

## update-gatekeeper-crds.sh

TBD

## update-grafana-dashboards.sh

TBD

## update-kubermatic-ca-bundle.sh

TBD

## update-prometheus-rules.sh

TBD

## update-velero-crds.sh

TBD

## verify-boilerplate.sh

TBD

## verify-codegen.sh

TBD

## verify-docs.sh

TBD

## verify-forbidden-functions.sh

TBD

## verify-grafana-dashboards.sh

TBD

## verify-import-order.sh

TBD

## verify-licenses.sh

TBD

## verify-links.sh

This script runs the lychee link checker against all Markdown, HTML,
Go and YAML files. This script is not part of the pre-verify Prowjob.

## verify-prometheus-rules.sh

TBD

## verify-spelling.sh

TBD

## versions-gen.sh

TBD

## run-kubermatic-webhook.sh

The script generates self-signed certificates and starts the Kubermatic webhook. The webhook can then be accessed via curl.

### Using the locally running webhook in your KKP setup

Testing/using the webhook via curl is not feasible and a painful experience. We recommend using this webhook in your KKP setup in the following way:

1. Use tools like ngrok to expose the webhook to the internet. After running the script, run the command: `ngrok http 443`
2. Create a Validating/Mutating WebhookConfiguration in your KKP setup similar to the following:

```yaml
# An example of a ValidatingWebhookConfiguration for the ApplicationDefinition resource.
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  name: kubermatic-application-definition
webhooks:
  - admissionReviewVersions:
      - v1
      - v1beta1
    clientConfig:
      # Replace with your ngrok URL
      url: https://redacted.ngrok-free.app/validate-application-definition
    failurePolicy: Fail
    matchPolicy: Equivalent
    name: applicationdefinitions.apps.kubermatic.k8c.io
    namespaceSelector: {}
    objectSelector:
      matchLabels:
        local-test: "true"
    rules:
      - apiGroups:
          - apps.kubermatic.k8c.io
        apiVersions:
          - "*"
        operations:
          - CREATE
          - UPDATE
          - DELETE
        resources:
          - applicationdefinitions
        scope: "*"
    sideEffects: None
    timeoutSeconds: 30
```

This is it, now all the validating requests will be sent to your webhook for ApplicatioDefinition resources with the label `local-test: "true"`. You can modify the webhook to match your needs.

**NOTE:** If your webhook configuration(ValidatingWebhookConfiguration or MutatingWebhookConfiguration) exists in the user cluster, you might need to add a network policy in the **user cluster namespace in seed** to allow the k8s API server to access the webhook. A general, not so nice and secure, example network policy is as follows:

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all-traffic
  namespace: cluster-xyz
spec:
  egress:
    - {}
  ingress:
    - {}
  podSelector: {}
  policyTypes:
    - Ingress
    - Egress
```
