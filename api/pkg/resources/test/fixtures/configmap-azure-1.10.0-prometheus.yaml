data:
  prometheus.yaml: |
    global:
      evaluation_interval: 30s
      scrape_interval: 30s
      external_labels:
        cluster: "de-test-01"
        seed_cluster: "testdc"

    rule_files:
    - "/etc/prometheus/config/rules*.yaml"

    alerting:
      alertmanagers:
      - dns_sd_configs:
        # configure the Seed's alertmanager for the user cluster
        - names:
          - 'alertmanager.monitoring.svc.cluster.local'
          type: A
          port: 9093

    scrape_configs:
    #######################################################################
    # These rules will scrape pods running inside the seed cluster.

    # scrape the etcd pods
    - job_name: etcd
      scheme: https
      tls_config:
        ca_file: /etc/etcd/pki/client/ca.crt
        cert_file: /etc/etcd/pki/client/apiserver-etcd-client.crt
        key_file: /etc/etcd/pki/client/apiserver-etcd-client.key

      static_configs:
      - targets:
        - 'etcd-0.etcd.cluster-de-test-01.svc.cluster.local:2379'
        - 'etcd-1.etcd.cluster-de-test-01.svc.cluster.local:2379'
        - 'etcd-2.etcd.cluster-de-test-01.svc.cluster.local:2379'

      relabel_configs:
      - source_labels: [__address__]
        regex: (etcd-\d+).+
        action: replace
        replacement: $1
        target_label: instance

    # scrape the cluster's control plane (apiserver, controller-manager, scheduler)
    - job_name: kubernetes-control-plane
      scheme: https
      tls_config:
        ca_file: /etc/kubernetes/ca.crt
        cert_file: /etc/kubernetes/prometheus-client.crt
        key_file: /etc/kubernetes/prometheus-client.key
        # insecure_skip_verify is needed because the apiservers certificate
        # does not contain a common name for the pod's ip address
        insecure_skip_verify: true

      kubernetes_sd_configs:
      - role: pod
        namespaces:
          names:
          - "cluster-de-test-01"

      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape_with_kube_cert]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: namespace
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        target_label: pod
      - source_labels: [__meta_kubernetes_pod_label_app]
        action: replace
        target_label: job

      # drop very expensive apiserver metrics
      metric_relabel_configs:
      - source_labels: [__name__]
        regex: 'apiserver_request_(duration|latencies)_.*'
        action: drop
      - source_labels: [__name__]
        regex: 'apiserver_response_sizes_.*'
        action: drop

    # scrape other cluster control plane components, like kube-state-metrics, DNS resolver,
    # machine-controller etcd.
    - job_name: control-plane-pods
      kubernetes_sd_configs:
      - role: pod
        namespaces:
          names:
          - "cluster-de-test-01"

      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_label_app, __meta_kubernetes_pod_container_init]
        regex: "kube-state-metrics;true"
        action: drop
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__
      - source_labels: [__meta_kubernetes_pod_label_role, __meta_kubernetes_pod_label_app]
        action: replace
        target_label: job
        separator: ''
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: namespace
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        target_label: pod

    #######################################################################
    # These rules will scrape pods running inside the user cluster itself.

    # scrape node metrics
    - job_name: nodes
      scheme: https
      tls_config:
        ca_file: /etc/kubernetes/ca.crt
        cert_file: /etc/kubernetes/prometheus-client.crt
        key_file: /etc/kubernetes/prometheus-client.key

      kubernetes_sd_configs:
      - role: node
        api_server: 'https://apiserver-external.cluster-de-test-01.svc.cluster.local.:30000'
        tls_config:
          ca_file: /etc/kubernetes/ca.crt
          cert_file: /etc/kubernetes/prometheus-client.crt
          key_file: /etc/kubernetes/prometheus-client.key

      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - target_label: __address__
        replacement: 'apiserver-external.cluster-de-test-01.svc.cluster.local.:30000'
      - source_labels: [__meta_kubernetes_node_name]
        regex: (.+)
        target_label: __metrics_path__
        replacement: /api/v1/nodes/${1}/proxy/metrics

    # scrape node cadvisor
    - job_name: cadvisor
      scheme: https
      tls_config:
        ca_file: /etc/kubernetes/ca.crt
        cert_file: /etc/kubernetes/prometheus-client.crt
        key_file: /etc/kubernetes/prometheus-client.key

      kubernetes_sd_configs:
      - role: node
        api_server: 'https://apiserver-external.cluster-de-test-01.svc.cluster.local.:30000'
        tls_config:
          ca_file: /etc/kubernetes/ca.crt
          cert_file: /etc/kubernetes/prometheus-client.crt
          key_file: /etc/kubernetes/prometheus-client.key

      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - target_label: __address__
        replacement: 'apiserver-external.cluster-de-test-01.svc.cluster.local.:30000'
      - source_labels: [__meta_kubernetes_node_name]
        regex: (.+)
        target_label: __metrics_path__
        replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor

    # scrape pods inside the user cluster with a special annotation
    - job_name: 'user-cluster-pods'
      scheme: https
      tls_config:
        ca_file: /etc/kubernetes/ca.crt
        cert_file: /etc/kubernetes/prometheus-client.crt
        key_file: /etc/kubernetes/prometheus-client.key

      kubernetes_sd_configs:
      - role: pod
        api_server: 'https://apiserver-external.cluster-de-test-01.svc.cluster.local.:30000'
        tls_config:
          ca_file: /etc/kubernetes/ca.crt
          cert_file: /etc/kubernetes/prometheus-client.crt
          key_file: /etc/kubernetes/prometheus-client.key

      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_kubermatic_io_monitoring_port]
        action: keep
        regex: \d+
      - source_labels: [__meta_kubernetes_pod_annotation_kubermatic_io_monitoring_path]
        regex: (.+)
        action: replace
        target_label: __metrics_path__
      - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_pod_name, __meta_kubernetes_pod_annotation_kubermatic_io_monitoring_port, __metrics_path__]
        action: replace
        regex: (.*);(.*);(.*);(.*)
        target_label: __metrics_path__
        replacement: /api/v1/namespaces/${1}/pods/${2}:${3}/proxy${4}
      - target_label: __address__
        replacement: 'apiserver-external.cluster-de-test-01.svc.cluster.local.:30000'
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: namespace
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        target_label: pod
    #######################################################################
    # custom scraping configurations

    - job_name: custom-test-config
      scheme: https
      metrics_path: '/metrics'
      static_configs:
      - targets:
        - 'foo.bar:12345'
  rules.yaml: |
    groups:
    - name: kubermatic.goprocess
      rules:
      - record: job:process_resident_memory_bytes:clone
        expr: process_resident_memory_bytes
        labels:
          kubermatic: federate

      - record: job:process_cpu_seconds_total:rate5m
        expr: rate(process_cpu_seconds_total[5m])
        labels:
          kubermatic: federate

      - record: job:process_open_fds:clone
        expr: process_open_fds
        labels:
          kubermatic: federate

    - name: kubermatic.machine_controller
      rules:
      - record: job:machine_controller_errors_total:rate5m
        expr: rate(machine_controller_errors_total[5m])
        labels:
          kubermatic: federate

      - alert: KubernetesAdmissionWebhookHighRejectionRate
        annotations:
          message: '{{ $labels.operation }} requests for Machine objects are failing (Admission) with a high rate. Consider checking the affected objects'
        expr: rate(apiserver_admission_webhook_admission_latencies_seconds_count{name="machine-controller.kubermatic.io-machines",rejected="true"}[5m]) > 0.01
        for: 5m
        labels:
          severity: warning

    - name: kubermatic.etcd
      rules:
      - record: job:etcd_server_has_leader:sum
        expr: sum(etcd_server_has_leader)
        labels:
          kubermatic: federate

      - record: job:etcd_disk_wal_fsync_duration_seconds_bucket:99percentile
        expr: histogram_quantile(0.99, sum(rate(etcd_disk_wal_fsync_duration_seconds_bucket[5m])) by (instance, le))
        labels:
          kubermatic: federate

      - record: job:etcd_disk_backend_commit_duration_seconds_bucket:99percentile
        expr: histogram_quantile(0.99, sum(rate(etcd_disk_backend_commit_duration_seconds_bucket[5m])) by (instance, le))
        labels:
          kubermatic: federate

      - record: job:etcd_debugging_mvcc_db_total_size_in_bytes:clone
        expr: etcd_debugging_mvcc_db_total_size_in_bytes
        labels:
          kubermatic: federate

      - record: job:etcd_network_client_grpc_received_bytes_total:rate5m
        expr: rate(etcd_network_client_grpc_received_bytes_total[5m])
        labels:
          kubermatic: federate

      - record: job:etcd_network_client_grpc_sent_bytes_total:rate5m
        expr: rate(etcd_network_client_grpc_sent_bytes_total[5m])
        labels:
          kubermatic: federate

      - record: job:etcd_network_peer_received_bytes_total:rate5msum
        expr: sum(rate(etcd_network_peer_received_bytes_total[5m])) by (instance)
        labels:
          kubermatic: federate

      - record: job:etcd_network_peer_sent_bytes_total:rate5msum
        expr: sum(rate(etcd_network_peer_sent_bytes_total[5m])) by (instance)
        labels:
          kubermatic: federate

      - record: job:etcd_server_proposals_failed_total:rate5msum
        expr: sum(rate(etcd_server_proposals_failed_total[5m]))
        labels:
          kubermatic: federate

      - record: job:etcd_server_proposals_pending:sum
        expr: sum(etcd_server_proposals_pending)
        labels:
          kubermatic: federate

      - record: job:etcd_server_proposals_committed_total:rate5msum
        expr: sum(rate(etcd_server_proposals_committed_total[5m]))
        labels:
          kubermatic: federate

      - record: job:etcd_server_proposals_applied_total:rate5msum
        expr: sum(rate(etcd_server_proposals_applied_total[5m]))
        labels:
          kubermatic: federate

      - record: job:etcd_server_leader_changes_seen_total:changes1d
        expr: changes(etcd_server_leader_changes_seen_total[1d])
        labels:
          kubermatic: federate

      - record: job:etcd_debugging_mvcc_delete_total:rate5m
        expr: rate(etcd_debugging_mvcc_delete_total[5m])
        labels:
          kubermatic: federate

      - record: job:etcd_debugging_mvcc_put_total:rate5m
        expr: rate(etcd_debugging_mvcc_put_total[5m])
        labels:
          kubermatic: federate

      - record: job:etcd_debugging_mvcc_range_total:rate5m
        expr: rate(etcd_debugging_mvcc_range_total[5m])
        labels:
          kubermatic: federate

      - record: job:etcd_debugging_mvcc_watcher_total:rate5m
        expr: rate(etcd_debugging_mvcc_watcher_total[5m])
        labels:
          kubermatic: federate

      - record: job:etcd_debugging_mvcc_txn_total:rate5m
        expr: rate(etcd_debugging_mvcc_txn_total[5m])
        labels:
          kubermatic: federate

      - record: job:etcd_debugging_mvcc_keys_total:clone
        expr: etcd_debugging_mvcc_keys_total
        labels:
          kubermatic: federate

      - record: job:etcd_debugging_store_reads_total:rate5m
        expr: rate(etcd_debugging_store_reads_total[5m])
        labels:
          kubermatic: federate

      - record: job:etcd_debugging_store_writes_total:rate5m
        expr: rate(etcd_debugging_store_writes_total[5m])
        labels:
          kubermatic: federate

      - record: job:etcd_debugging_store_expires_total:rate5m
        expr: rate(etcd_debugging_store_expires_total[5m])
        labels:
          kubermatic: federate

    - name: machine-controller
      rules:
      - alert: MachineControllerTooManyErrors
        annotations:
          message: Machine Controller in {{ $labels.namespace }} has too many errors in its loop.
        expr: |
          sum(rate(machine_controller_errors_total[5m])) by (namespace) > 0.01
        for: 20m
        labels:
          severity: warning

      - alert: MachineControllerMachineDeletionTakesTooLong
        annotations:
          message: Machine {{ $labels.machine }} of cluster {{ $labels.cluster }} is stuck in deletion for more than 30min.
        expr: (time() - machine_controller_machine_deleted) > 30*60
        for: 0m
        labels:
          severity: warning

      - alert: AWSInstanceCountTooHigh
        annotations:
          message: '{{ $labels.machine }} has more than one instance at AWS'
        expr: machine_controller_aws_instances_for_machine > 1
        for: 30m
        labels:
          severity: warning

      - record: job:machine_controller_errors_total:rate5m
        expr: rate(machine_controller_errors_total[5m])
        labels:
          kubermatic: federate

      - record: job:machine_controller_workers:sum
        expr: sum(machine_controller_workers)
        labels:
          kubermatic: federate

      - record: job:machine_controller_machines:sum
        expr: sum(machine_controller_machines)
        labels:
          kubermatic: federate

    - name: etcd
      rules:
      - alert: EtcdInsufficientMembers
        annotations:
          message: 'Etcd cluster "{{ $labels.job }}": insufficient members ({{ $value }}).'
        expr: |
          sum(up{job="etcd"} == bool 1) by (job) < ((count(up{job="etcd"}) by (job) + 1) / 2)
        for: 15m
        labels:
          severity: critical

      - alert: EtcdNoLeader
        annotations:
          message: 'Etcd cluster "{{ $labels.job }}": member {{ $labels.instance }} has no leader.'
        expr: |
          etcd_server_has_leader{job="etcd"} == 0
        for: 15m
        labels:
          severity: critical

      - alert: EtcdHighNumberOfLeaderChanges
        annotations:
          message: 'Etcd cluster "{{ $labels.job }}": instance {{ $labels.instance }} has seen {{ $value }} leader changes within the last hour.'
        expr: |
          rate(etcd_server_leader_changes_seen_total{job="etcd"}[15m]) > 3
        for: 15m
        labels:
          severity: warning

      - alert: EtcdGRPCRequestsSlow
        annotations:
          message: 'Etcd cluster "{{ $labels.job }}": gRPC requests to {{ $labels.grpc_method }} are taking {{ $value }}s on etcd instance {{ $labels.instance }}.'
        expr: |
          histogram_quantile(0.99, sum(rate(grpc_server_handling_seconds_bucket{job="etcd", grpc_type="unary"}[5m])) by (job, instance, grpc_service, grpc_method, le))
          > 0.15
        for: 10m
        labels:
          severity: critical

      - alert: EtcdMemberCommunicationSlow
        annotations:
          message: 'Etcd cluster "{{ $labels.job }}": member communication with {{ $labels.To }} is taking {{ $value }}s on etcd instance {{ $labels.instance }}.'
        expr: |
          histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket{job="etcd"}[5m]))
          > 0.15
        for: 10m
        labels:
          severity: warning

      - alert: EtcdHighNumberOfFailedProposals
        annotations:
          message: 'Etcd cluster "{{ $labels.job }}": {{ $value }} proposal failures within the last hour on etcd instance {{ $labels.instance }}.'
        expr: |
          rate(etcd_server_proposals_failed_total{job="etcd"}[15m]) > 5
        for: 15m
        labels:
          severity: warning

      - alert: EtcdHighFsyncDurations
        annotations:
          message: 'Etcd cluster "{{ $labels.job }}": 99th percentile fync durations are {{ $value }}s on etcd instance {{ $labels.instance }}.'
        expr: |
          histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket{job="etcd"}[5m]))
          > 0.5
        for: 10m
        labels:
          severity: warning

      - alert: EtcdHighCommitDurations
        annotations:
          message: 'Etcd cluster "{{ $labels.job }}": 99th percentile commit durations {{ $value }}s on etcd instance {{ $labels.instance }}.'
        expr: |
          histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket{job="etcd"}[5m]))
          > 0.25
        for: 10m
        labels:
          severity: warning

    - name: process.filedescriptors
      rules:
      - expr: process_open_fds / process_max_fds
        record: instance:fd_utilization

      - alert: FdExhaustionClose
        annotations:
          message: '{{ $labels.job }} instance {{ $labels.instance }} will exhaust its file descriptors soon'
        expr: |
          predict_linear(instance:fd_utilization[1h], 3600 * 4) > 1
        for: 10m
        labels:
          severity: warning

      - alert: FdExhaustionClose
        annotations:
          message: '{{ $labels.job }} instance {{ $labels.instance }} will exhaust its file descriptors soon'
        expr: |
          predict_linear(instance:fd_utilization[10m], 3600) > 1
        for: 10m
        labels:
          severity: critical

    - name: kubernetes-absent
      rules:
      - alert: KubernetesApiserverDown
        annotations:
          message: Kubernetes apiserver has disappeared from Prometheus target discovery.
        expr: absent(up{job="apiserver"} == 1)
        for: 15m
        labels:
          severity: critical

      - alert: KubernetesControllerManagerDown
        annotations:
          message: Kubernetes controller-manager has disappeared from Prometheus target discovery.
        expr: absent(up{job="controller-manager"} == 1) and absent(up{job="openshift-controller-manager"} == 1)
        for: 15m
        labels:
          severity: critical

      - alert: KubernetesSchedulerDown
        annotations:
          message: Kubernetes scheduler has disappeared from Prometheus target discovery.
        expr: absent(up{job="scheduler"} == 1) and absent(up{job="openshift-controller-manager"} == 1)
        for: 15m
        labels:
          severity: critical

      - alert: MachineControllerDown
        annotations:
          message: Machine controller has disappeared from Prometheus target discovery.
        expr: absent(up{job="machine-controller"} == 1)
        for: 15m
        labels:
          severity: critical

      - alert: UserClusterControllerDown
        annotations:
          message: User Cluster Controller has disappeared from Prometheus target discovery.
        expr: absent(up{job="usercluster-controller"} == 1)
        for: 15m
        labels:
          severity: critical

      - alert: KubeStateMetricsDown
        annotations:
          message: Kube-state-metrics has disappeared from Prometheus target discovery.
        expr: absent(up{job="kube-state-metrics"} == 1)
        for: 15m
        labels:
          severity: warning

      - alert: EtcdDown
        annotations:
          message: Etcd has disappeared from Prometheus target discovery.
        expr: absent(up{job="etcd"} == 1)
        for: 15m
        labels:
          severity: critical

      # This is triggered if the cluster does have nodes, but the cadvisor could
      # not successfully be scraped for whatever reason. An absent() on cadvisor
      # metrics is not a good alert because clusters could simply have no nodes
      # and hence no cadvisors.
      - alert: CAdvisorDown
        annotations:
          message: cAdvisor on {{ $labels.kubernetes_io_hostname }} could not be scraped.
        expr: up{job="cadvisor"} == 0
        for: 15m
        labels:
          severity: warning

      # This functions similarly to the cadvisor alert above.
      - alert: KubernetesNodeDown
        annotations:
          message: The kubelet on {{ $labels.kubernetes_io_hostname }} could not be scraped.
        expr: up{job="kubernetes-nodes"} == 0
        for: 15m
        labels:
          severity: warning

      - alert: DNSResolverDown
        annotations:
          message: DNS resolver has disappeared from Prometheus target discovery.
        expr: absent(up{job="dns-resolver"} == 1)
        for: 15m
        labels:
          severity: warning

    - name: kubernetes-nodes
      rules:
      - alert: KubernetesNodeNotReady
        annotations:
          message: '{{ $labels.node }} has been unready for more than an hour.'
        expr: kube_node_status_condition{condition="Ready",status="true"} == 0
        for: 30m
        labels:
          severity: warning
metadata:
  creationTimestamp: null
  labels:
    app: prometheus
