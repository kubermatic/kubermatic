data:
  prometheus.yaml: |
    global:
      evaluation_interval: 30s
      scrape_interval: 30s
      external_labels:
        cluster: "de-test-01"
        seed_cluster: "testdc"
    rule_files:
    - "/etc/prometheus/config/rules*.yaml"
    scrape_configs:
    some test
    - job_name: etcd
      scheme: https
      metrics_path: '/metrics'
      static_configs:
      - targets:
        - 'etcd-0.etcd.cluster-de-test-01.svc.cluster.local:2379'
        - 'etcd-1.etcd.cluster-de-test-01.svc.cluster.local:2379'
        - 'etcd-2.etcd.cluster-de-test-01.svc.cluster.local:2379'
      tls_config:
        ca_file: /etc/etcd/pki/client/ca.crt
        cert_file: /etc/etcd/pki/client/apiserver-etcd-client.crt
        key_file: /etc/etcd/pki/client/apiserver-etcd-client.key
      relabel_configs:
      - source_labels: [__address__]
        regex: (etcd-\d+).+
        action: replace
        replacement: $1
        target_label: instance

    - job_name: 'kubernetes-nodes'
      scheme: https
      tls_config:
        ca_file: /etc/kubernetes/ca.crt
        cert_file: /etc/kubernetes/prometheus-client.crt
        key_file: /etc/kubernetes/prometheus-client.key

      kubernetes_sd_configs:
      - role: node
        api_server: 'apiserver-external.cluster-de-test-01.svc.cluster.local.:30000'
        tls_config:
          ca_file: /etc/kubernetes/ca.crt
          cert_file: /etc/kubernetes/prometheus-client.crt
          key_file: /etc/kubernetes/prometheus-client.key

      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - target_label: __address__
        replacement: 'apiserver-external.cluster-de-test-01.svc.cluster.local.:30000'
      - source_labels: [__meta_kubernetes_node_name]
        regex: (.+)
        target_label: __metrics_path__
        replacement: /api/v1/nodes/${1}/proxy/metrics

    - job_name: 'apiservers'
      kubernetes_sd_configs:
      - role: pod
        namespaces:
          names:
          - "cluster-de-test-01"
      scheme: https
      tls_config:
        ca_file: /etc/kubernetes/ca.crt
        cert_file: /etc/kubernetes/prometheus-client.crt
        key_file: /etc/kubernetes/prometheus-client.key
        # insecure_skip_verify is needed because the apiservers certificate
        # does not contain a common name for the pods ip address
        insecure_skip_verify: true
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape_apiserver]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: namespace
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        target_label: pod

    - job_name: 'user-cluster-pods'
      scheme: https
      tls_config:
        ca_file: /etc/kubernetes/ca.crt
        cert_file: /etc/kubernetes/prometheus-client.crt
        key_file: /etc/kubernetes/prometheus-client.key
      kubernetes_sd_configs:
      - role: pod
        api_server: 'apiserver-external.cluster-de-test-01.svc.cluster.local.:30000'
        tls_config:
          ca_file: /etc/kubernetes/ca.crt
          cert_file: /etc/kubernetes/prometheus-client.crt
          key_file: /etc/kubernetes/prometheus-client.key
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_tmpkubermatic410329466_port]
        action: keep
        regex: \d+
      - source_labels: [__meta_kubernetes_pod_annotation_tmpkubermatic410329466_path]
        regex: (.+)
        action: replace
        target_label: __metrics_path__
      - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_pod_name, __meta_kubernetes_pod_annotation_tmpkubermatic410329466_port, __metrics_path__]
        action: replace
        regex: (.*);(.*);(.*);(.*)
        target_label: __metrics_path__
        replacement: /api/v1/namespaces/${1}/pods/${2}:${3}/proxy${4}
      - target_label: __address__
        replacement: 'apiserver-external.cluster-de-test-01.svc.cluster.local.:30000'
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: namespace
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        target_label: pod

    - job_name: 'control-plane-service-endpoints'

      kubernetes_sd_configs:
      - role: endpoints
        namespaces:
          names:
          - "cluster-de-test-01"

      relabel_configs:
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_endpoint_ready]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__
      - source_labels: [__meta_kubernetes_endpoint_name]
        action: replace
        target_label: job
      - source_labels: [__meta_kubernetes_endpoint_port_name]
        action: replace
        target_label: port_name
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: namespace
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        target_label: pod

    - job_name: 'control-plane-pods'

      kubernetes_sd_configs:
      - role: pod
        namespaces:
          names:
          - "cluster-de-test-01"

      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__
      - source_labels: [__meta_kubernetes_pod_label_role, __meta_kubernetes_pod_label_app]
        action: replace
        target_label: job
        separator: ''
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: namespace
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        target_label: pod
    alerting:
      alertmanagers:
      - dns_sd_configs:
        - names:
          - 'alertmanager-kubermatic.monitoring.svc.cluster.local'
          type: A
          port: 9093
  rules.yaml: |2

    groups:
    - name: kubermatic.goprocess
      rules:
      - record: job:process_resident_memory_bytes:clone
        expr: process_resident_memory_bytes
        labels:
          kubermatic: federate

      - record: job:process_cpu_seconds_total:rate5m
        expr: rate(process_cpu_seconds_total[5m])
        labels:
          kubermatic: federate

      - record: job:process_open_fds:clone
        expr: process_open_fds
        labels:
          kubermatic: federate

    - name: kubermatic.machine_controller
      rules:
      - record: job:machine_controller_errors_total:rate5m
        expr: rate(machine_controller_errors_total[5m])
        labels:
          kubermatic: federate

    - name: kubermatic.etcd
      rules:
      - record: job:etcd_server_has_leader:sum
        expr: sum(etcd_server_has_leader)
        labels:
          kubermatic: federate

      - record: job:etcd_disk_wal_fsync_duration_seconds_bucket:99percentile
        expr: histogram_quantile(0.99, sum(rate(etcd_disk_wal_fsync_duration_seconds_bucket[5m])) by (instance, le))
        labels:
          kubermatic: federate

      - record: job:etcd_disk_backend_commit_duration_seconds_bucket:99percentile
        expr: histogram_quantile(0.99, sum(rate(etcd_disk_backend_commit_duration_seconds_bucket[5m])) by (instance, le))
        labels:
          kubermatic: federate

      - record: job:etcd_debugging_mvcc_db_total_size_in_bytes:clone
        expr: etcd_debugging_mvcc_db_total_size_in_bytes
        labels:
          kubermatic: federate

      - record: job:etcd_network_client_grpc_received_bytes_total:rate5m
        expr: rate(etcd_network_client_grpc_received_bytes_total[5m])
        labels:
          kubermatic: federate

      - record: job:etcd_network_client_grpc_sent_bytes_total:rate5m
        expr: rate(etcd_network_client_grpc_sent_bytes_total[5m])
        labels:
          kubermatic: federate

      - record: job:etcd_network_peer_received_bytes_total:rate5msum
        expr: sum(rate(etcd_network_peer_received_bytes_total[5m])) by (instance)
        labels:
          kubermatic: federate

      - record: job:etcd_network_peer_sent_bytes_total:rate5msum
        expr: sum(rate(etcd_network_peer_sent_bytes_total[5m])) by (instance)
        labels:
          kubermatic: federate

      - record: job:etcd_server_proposals_failed_total:rate5msum
        expr: sum(rate(etcd_server_proposals_failed_total[5m]))
        labels:
          kubermatic: federate

      - record: job:etcd_server_proposals_pending:sum
        expr: sum(etcd_server_proposals_pending)
        labels:
          kubermatic: federate

      - record: job:etcd_server_proposals_committed_total:rate5msum
        expr: sum(rate(etcd_server_proposals_committed_total[5m]))
        labels:
          kubermatic: federate

      - record: job:etcd_server_proposals_applied_total:rate5msum
        expr: sum(rate(etcd_server_proposals_applied_total[5m]))
        labels:
          kubermatic: federate

      - record: job:etcd_server_leader_changes_seen_total:changes1d
        expr: changes(etcd_server_leader_changes_seen_total[1d])
        labels:
          kubermatic: federate

      - record: job:etcd_debugging_mvcc_delete_total:rate5m
        expr: rate(etcd_debugging_mvcc_delete_total[5m])
        labels:
          kubermatic: federate

      - record: job:etcd_debugging_mvcc_put_total:rate5m
        expr: rate(etcd_debugging_mvcc_put_total[5m])
        labels:
          kubermatic: federate

      - record: job:etcd_debugging_mvcc_range_total:rate5m
        expr: rate(etcd_debugging_mvcc_range_total[5m])
        labels:
          kubermatic: federate

      - record: job:etcd_debugging_mvcc_watcher_total:rate5m
        expr: rate(etcd_debugging_mvcc_watcher_total[5m])
        labels:
          kubermatic: federate

      - record: job:etcd_debugging_mvcc_txn_total:rate5m
        expr: rate(etcd_debugging_mvcc_txn_total[5m])
        labels:
          kubermatic: federate

      - record: job:etcd_debugging_mvcc_keys_total:clone
        expr: etcd_debugging_mvcc_keys_total
        labels:
          kubermatic: federate

      - record: job:etcd_debugging_store_reads_total:rate5m
        expr: rate(etcd_debugging_store_reads_total[5m])
        labels:
          kubermatic: federate

      - record: job:etcd_debugging_store_writes_total:rate5m
        expr: rate(etcd_debugging_store_writes_total[5m])
        labels:
          kubermatic: federate

      - record: job:etcd_debugging_store_expires_total:rate5m
        expr: rate(etcd_debugging_store_expires_total[5m])
        labels:
          kubermatic: federate

    - name: machine-controller
      rules:
      - alert: MachineControllerTooManyErrors
        annotations:
          message: Machine Controller in {{ $labels.namespace }} has too many errors in
            its loop.
        expr: |
          sum(rate(machine_controller_errors_total[5m])) by (namespace) > 0.01
        for: 10m
        labels:
          severity: warning
      - alert: MachineControllerMachineDeletionTakesTooLong
        annotations:
          message: Machine {{ $labels.machine }} of cluster {{ $labels.cluster }} is stuck
            in deletion for more than 30min.
        expr: (time() - machine_controller_machine_deleted) > 30*60
        for: 0m
        labels:
          severity: warning
    - name: etcd
      rules:
      - alert: EtcdInsufficientMembers
        annotations:
          message: 'Etcd cluster "{{ $labels.job }}": insufficient members ({{ $value
            }}).'
        expr: |
          sum(up{job=~".*etcd.*"} == bool 1) by (job) < ((count(up{job=~".*etcd.*"}) by (job) + 1) / 2)
        for: 3m
        labels:
          severity: critical
      - alert: EtcdNoLeader
        annotations:
          message: 'Etcd cluster "{{ $labels.job }}": member {{ $labels.instance }} has
            no leader.'
        expr: |
          etcd_server_has_leader{job=~".*etcd.*"} == 0
        for: 1m
        labels:
          severity: critical
      - alert: EtcdHighNumberOfLeaderChanges
        annotations:
          message: 'Etcd cluster "{{ $labels.job }}": instance {{ $labels.instance }}
            has seen {{ $value }} leader changes within the last hour.'
        expr: |
          rate(etcd_server_leader_changes_seen_total{job=~".*etcd.*"}[15m]) > 3
        for: 15m
        labels:
          severity: warning
      - alert: EtcdHighNumberOfFailedGRPCRequests
        annotations:
          message: 'Etcd cluster "{{ $labels.job }}": {{ $value }}% of requests for {{
            $labels.grpc_method }} failed on etcd instance {{ $labels.instance }}.'
        expr: |
          100 * sum(rate(grpc_server_handled_total{job=~".*etcd.*", grpc_code!="OK"}[5m])) BY (job, instance, grpc_service, grpc_method)
            /
          sum(rate(grpc_server_handled_total{job=~".*etcd.*"}[5m])) BY (job, instance, grpc_service, grpc_method)
            > 1
        for: 10m
        labels:
          severity: warning
      - alert: EtcdHighNumberOfFailedGRPCRequests
        annotations:
          message: 'Etcd cluster "{{ $labels.job }}": {{ $value }}% of requests for {{
            $labels.grpc_method }} failed on etcd instance {{ $labels.instance }}.'
        expr: |
          100 * sum(rate(grpc_server_handled_total{job=~".*etcd.*", grpc_code!="OK"}[5m])) BY (job, instance, grpc_service, grpc_method)
            /
          sum(rate(grpc_server_handled_total{job=~".*etcd.*"}[5m])) BY (job, instance, grpc_service, grpc_method)
            > 5
        for: 5m
        labels:
          severity: critical
      - alert: EtcdGRPCRequestsSlow
        annotations:
          message: 'Etcd cluster "{{ $labels.job }}": gRPC requests to {{ $labels.grpc_method
            }} are taking {{ $value }}s on etcd instance {{ $labels.instance }}.'
        expr: |
          histogram_quantile(0.99, sum(rate(grpc_server_handling_seconds_bucket{job=~".*etcd.*", grpc_type="unary"}[5m])) by (job, instance, grpc_service, grpc_method, le))
          > 0.15
        for: 10m
        labels:
          severity: critical
      - alert: EtcdMemberCommunicationSlow
        annotations:
          message: 'Etcd cluster "{{ $labels.job }}": member communication with {{ $labels.To
            }} is taking {{ $value }}s on etcd instance {{ $labels.instance }}.'
        expr: |
          histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket{job=~".*etcd.*"}[5m]))
          > 0.15
        for: 10m
        labels:
          severity: warning
      - alert: EtcdHighNumberOfFailedProposals
        annotations:
          message: 'Etcd cluster "{{ $labels.job }}": {{ $value }} proposal failures within
            the last hour on etcd instance {{ $labels.instance }}.'
        expr: |
          rate(etcd_server_proposals_failed_total{job=~".*etcd.*"}[15m]) > 5
        for: 15m
        labels:
          severity: warning
      - alert: EtcdHighFsyncDurations
        annotations:
          message: 'Etcd cluster "{{ $labels.job }}": 99th percentile fync durations are
            {{ $value }}s on etcd instance {{ $labels.instance }}.'
        expr: |
          histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket{job=~".*etcd.*"}[5m]))
          > 0.5
        for: 10m
        labels:
          severity: warning
      - alert: EtcdHighCommitDurations
        annotations:
          message: 'Etcd cluster "{{ $labels.job }}": 99th percentile commit durations
            {{ $value }}s on etcd instance {{ $labels.instance }}.'
        expr: |
          histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket{job=~".*etcd.*"}[5m]))
          > 0.25
        for: 10m
        labels:
          severity: warning
      - expr: process_open_fds / process_max_fds
        record: instance:fd_utilization
      - alert: FdExhaustionClose
        annotations:
          message: '{{ $labels.job }} instance {{ $labels.instance }} will exhaust its
            file descriptors soon'
        expr: |
          predict_linear(instance:fd_utilization{job=~".*etcd.*"}[1h], 3600 * 4) > 1
        for: 10m
        labels:
          severity: warning
      - alert: FdExhaustionClose
        annotations:
          description: '{{ $labels.job }} instance {{ $labels.instance }} will exhaust
            its file descriptors soon'
        expr: |
          predict_linear(instance:fd_utilization{job=~".*etcd.*"}[10m], 3600) > 1
        for: 10m
        labels:
          severity: critical
metadata:
  creationTimestamp: null
  labels:
    app: prometheus
  name: prometheus
  ownerReferences:
  - apiVersion: kubermatic.k8s.io/v1
    blockOwnerDeletion: true
    controller: true
    kind: Cluster
    name: de-test-01
    uid: "1234567890"
