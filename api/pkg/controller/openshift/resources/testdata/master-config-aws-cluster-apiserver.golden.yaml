admissionConfig:
  pluginConfig:
    BuildDefaults:
      configuration:
        apiVersion: v1
        env: []
        kind: BuildDefaultsConfig
        resources:
          limits: {}
          requests: {}
    BuildOverrides:
      configuration:
        apiVersion: v1
        kind: BuildOverridesConfig
    openshift.io/ImagePolicy:
      configuration:
        apiVersion: v1
        executionRules:
        - matchImageAnnotations:
          - key: images.openshift.io/deny-execution
            value: 'true'
          name: execution-denied
          onResources:
          - resource: pods
          - resource: builds
          reject: true
          skipOnResolutionFailure: true
        kind: ImagePolicyConfig
aggregatorConfig:
  proxyClientInfo:
    certFile: /etc/kubernetes/pki/front-proxy/client/apiserver-proxy-client.crt
    keyFile: /etc/kubernetes/pki/front-proxy/client/apiserver-proxy-client.key
apiLevels:
- v1
apiVersion: v1
authConfig:
  requestHeader:
    clientCA: /etc/kubernetes/pki/front-proxy/client/ca.crt
    clientCommonNames:
    - aggregator-front-proxy
    extraHeaderPrefixes:
    - X-Remote-Extra-
    groupHeaders:
    - X-Remote-Group
    usernameHeaders:
    - X-Remote-User
controllerConfig:
  election:
    lockName: openshift-master-controllers
  serviceServingCert:
    signer:
     # CA used for signing serving certs on demand for user workloads
     # https://github.com/openshift/service-serving-cert-signer
     # In theory this shouldn't be required, in practise the APIServer
     # panics on startup if not passed
     certFile: /etc/origin/master/service-signer-ca/ca.crt
     keyFile: /etc/origin/master/service-signer-ca/ca.key
controllers: '*'
corsAllowedOrigins:
## TODO: Fix up to contain all public addresses
- (?i)//127\.0\.0\.1(:|\z)
- (?i)//localhost(:|\z)
- (?i)//kubernetes\.default(:|\z)
- (?i)//kubernetes\.default\.svc\.cluster\.local(:|\z)
- (?i)//kubernetes(:|\z)
- (?i)//openshift\.default(:|\z)
- (?i)//openshift\.default\.svc(:|\z)
- (?i)//172\.30\.0\.1(:|\z)
- (?i)//openshift\.default\.svc\.cluster\.local(:|\z)
- (?i)//kubernetes\.default\.svc(:|\z)
- (?i)//openshift(:|\z)
dnsConfig:
  bindAddress: 0.0.0.0:8053
  bindNetwork: tcp4
etcdClientInfo:
  ca: /etc/etcd/pki/client/ca.crt
  certFile: /etc/etcd/pki/client/apiserver-etcd-client.crt
  keyFile: /etc/etcd/pki/client/apiserver-etcd-client.key
  # Mandatory field, controller manager fails startup if unset
  urls: 
  - "https://etcd-0.etcd.cluster-aws-1.svc.cluster.local.:2379"
  - "https://etcd-1.etcd.cluster-aws-1.svc.cluster.local.:2379"
  - "https://etcd-2.etcd.cluster-aws-1.svc.cluster.local.:2379"
etcdStorageConfig:
  kubernetesStoragePrefix: kubernetes.io
  kubernetesStorageVersion: v1
  openShiftStoragePrefix: openshift.io
  openShiftStorageVersion: v1
imageConfig:
  format: docker.io/openshift/origin-${component}:${version}
  latest: false
imagePolicyConfig:
  internalRegistryHostname: docker-registry.default.svc:5000
kind: MasterConfig
kubeletClientInfo:
  ca: /etc/kubernetes/pki/ca/ca.crt
  certFile: /etc/kubernetes/kubelet/kubelet-client.crt
  keyFile: /etc/kubernetes/kubelet/kubelet-client.key
  # Port is required for the controller manager to start up
  port: 10250
kubernetesMasterConfig:
  apiServerArguments:
    storage-backend:
    - etcd3
    storage-media-type:
    - application/vnd.kubernetes.protobuf
    kubelet-preferred-address-types:
    - ExternalIP
    - InternalIP
    cloud-provider:
      - "aws"
    cloud-config:
      - "/etc/kubernetes/cloud/config"
  controllerArguments:
    cluster-signing-cert-file:
    - /etc/kubernetes/pki/ca/ca.crt
    cluster-signing-key-file:
    - /etc/kubernetes/pki/ca/ca.key
    pv-recycler-pod-template-filepath-hostpath:
    - /etc/origin/master/recycler_pod.yaml
    pv-recycler-pod-template-filepath-nfs:
    - /etc/origin/master/recycler_pod.yaml
    cloud-provider:
      - "aws"
    cloud-config:
      - "/etc/kubernetes/cloud/config"
  # For some reason this field results in an error: Encountered config error json: unknown field "masterCount" in object *config.MasterConfig, raw JSON:
  #masterCount: 1
  masterIP: "35.198.93.90"
  podEvictionTimeout: null
  proxyClientInfo:
    certFile: /etc/kubernetes/pki/front-proxy/client/apiserver-proxy-client.crt
    keyFile: /etc/kubernetes/pki/front-proxy/client/apiserver-proxy-client.key
  schedulerArguments: null
  schedulerConfigFile: /etc/origin/master/scheduler.json
  servicesNodePortRange: ''
  servicesSubnet: "10.240.16.0/24"
masterClients:
  externalKubernetesClientConnectionOverrides:
    acceptContentTypes: application/vnd.kubernetes.protobuf,application/json
    burst: 400
    contentType: application/vnd.kubernetes.protobuf
    qps: 200
  externalKubernetesKubeConfig: ''
  openshiftLoopbackClientConnectionOverrides:
    acceptContentTypes: application/vnd.kubernetes.protobuf,application/json
    burst: 600
    contentType: application/vnd.kubernetes.protobuf
    qps: 300
  openshiftLoopbackKubeConfig: /etc/origin/master/loopback-kubeconfig/kubeconfig
masterPublicURL: "https://henrik-os1.europe-west3-c.dev.kubermatic.io:32593"
networkConfig:
  clusterNetworks:
  - cidr: "172.25.0.0/16"
    # The number of bits to allocate per node subnet, e.G. 8 == hosts get a /24
    hostSubnetLength: 8
  externalIPNetworkCIDRs:
  - 0.0.0.0/0
  networkPluginName: redhat/openshift-ovs-subnet
  serviceNetworkCIDR: "10.240.16.0/24"
# TODO: Get this running with dex
oauthConfig:
  alwaysShowProviderSelection: false
  grantConfig:
    method: auto
    serviceAccountMethod: prompt
  identityProviders:
    - challenge: true
      login: true
      mappingMethod: claim
      name: openid-connect
      provider:
        apiVersion: v1
        kind: OpenIDIdentityProvider
        clientID: client-id
        clientSecret: client-secret
        ca: /etc/kubernetes/dex/ca/caBundle.pem
        claims:
          id:
            - sub
          preferredUsername:
            - name
          name:
            - name
          email:
            - email
        extraScopes:
          - email
          - profile
        urls:
          authorize: https://dev.kubermatic.io/dex/auth
          token: https://dev.kubermatic.io/dex/token
  masterCA: /etc/kubernetes/pki/ca/ca.crt
  assetPublicURL: "https://henrik-os1.europe-west3-c.dev.kubermatic.io:32593/console"
  masterPublicURL: "https://henrik-os1.europe-west3-c.dev.kubermatic.io:32593"
  masterURL: "https://henrik-os1.europe-west3-c.dev.kubermatic.io:32593"
  sessionConfig:
    sessionMaxAgeSeconds: 300
    sessionName: ssn
    sessionSecretsFile: ""
  templates: null
  tokenConfig:
    accessTokenMaxAgeSeconds: 86400
    authorizeTokenMaxAgeSeconds: 300
pauseControllers: false
policyConfig:
  bootstrapPolicyFile: /etc/origin/master/policy.json
  openshiftInfrastructureNamespace: openshift-infra
  openshiftSharedResourcesNamespace: openshift
projectConfig:
  defaultNodeSelector: node-role.kubernetes.io/compute=true
  projectRequestMessage: ''
  projectRequestTemplate: ''
  securityAllocator:
    mcsAllocatorRange: s0:/2
    mcsLabelsPerProject: 5
    uidAllocatorRange: 1000000000-1999999999/10000
routingConfig:
  subdomain: router.default.svc.cluster.local
serviceAccountConfig:
  limitSecretReferences: false
  managedNames:
  - default
  - builder
  - deployer
  masterCA: /etc/kubernetes/pki/ca/ca.crt
  privateKeyFile: /etc/kubernetes/service-account-key/sa.key
  publicKeyFiles:
  - /etc/kubernetes/service-account-key/sa.pub
servingInfo:
  bindAddress: 0.0.0.0:32593
  bindNetwork: tcp4
  clientCA: /etc/kubernetes/pki/ca/ca.crt
  certFile: /etc/kubernetes/tls/apiserver-tls.crt
  keyFile: /etc/kubernetes/tls/apiserver-tls.key
  maxRequestsInFlight: 500
  requestTimeoutSeconds: 3600
volumeConfig:
  dynamicProvisioningEnabled: true