package controller

import (
	"context"
	"fmt"
	"strings"

	"github.com/spf13/cobra"
	"go.uber.org/zap"

	"k8s.io/api/core/v1"
	kerrors "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/util/intstr"
	"k8s.io/client-go/tools/record"
	ctrlruntimeclient "sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/controller"
	"sigs.k8s.io/controller-runtime/pkg/event"
	"sigs.k8s.io/controller-runtime/pkg/handler"
	"sigs.k8s.io/controller-runtime/pkg/manager"
	"sigs.k8s.io/controller-runtime/pkg/predicate"
	"sigs.k8s.io/controller-runtime/pkg/reconcile"
	"sigs.k8s.io/controller-runtime/pkg/source"
)

const (
	controllerName = "cluster_exposer_controller"
)

type reconciler struct {
	ctx         context.Context
	log         *zap.SugaredLogger
	client      ctrlruntimeclient.Client
	outerClient ctrlruntimeclient.Client
	recorder    record.EventRecorder
	namespace   string
	kubeconfig  *string
	cmd         *cobra.Command
	jobID       string
}

func Add(ctx context.Context, outerClient ctrlruntimeclient.Client, log *zap.SugaredLogger, mgr manager.Manager, namespace string, kubeconfig *string, cmd *cobra.Command, jobID string) error {
	log = log.Named(controllerName)

	r := &reconciler{
		ctx:         ctx,
		log:         log,
		client:      mgr.GetClient(),
		outerClient: outerClient,
		recorder:    mgr.GetEventRecorderFor(controllerName),
		namespace:   namespace,
		kubeconfig:  kubeconfig,
		cmd:         cmd,
		jobID:       jobID,
	}
	c, err := controller.New(controllerName, mgr, controller.Options{
		Reconciler: r,
	})
	if err != nil {
		return fmt.Errorf("failed to create controller: %v", err)
	}
	// Watch for changes
	if err = c.Watch(&source.Kind{Type: &v1.Pod{}}, &handler.EnqueueRequestForObject{}, clusterAPI()); err != nil {
		return fmt.Errorf("failed to establish watch for the Pod %v", err)
	}

	return nil
}

func (r *reconciler) Reconcile(request reconcile.Request) (reconcile.Result, error) {
	log := r.log.With("Pod", request.Name)
	log.Debug("Reconciling")

	pod := &v1.Pod{}
	if err := r.client.Get(r.ctx, request.NamespacedName, pod); err != nil {
		if kerrors.IsNotFound(err) {
			log.Debug("pod deleted, returning")
			return reconcile.Result{}, err
		}
		return reconcile.Result{}, fmt.Errorf("failed to get pod: %v", err)
	}

	err := r.reconcile(log, pod)
	if err != nil {
		log.Errorw("Reconciling failed", zap.Error(err))
		r.recorder.Event(pod, v1.EventTypeWarning, "ExposingClusterFailed", err.Error())
	}
	return reconcile.Result{}, err
}

func (r *reconciler) reconcile(log *zap.SugaredLogger, pod *v1.Pod) error {

	outerService := &v1.Service{}
	if err := r.outerClient.Get(r.ctx, ctrlruntimeclient.ObjectKey{Namespace: r.namespace, Name: pod.Namespace}, outerService); err != nil {
		if kerrors.IsNotFound(err) {
			outerService, err = r.createOuterService(pod, r.jobID)
			if err != nil {
				return err
			}
		} else {
			return fmt.Errorf("failed to get outer service: %v", err)
		}
	}

	service := &v1.Service{}
	if err := r.client.Get(r.ctx, ctrlruntimeclient.ObjectKey{Namespace: pod.Namespace, Name: UserClusterAPIServerServiceName}, service); err != nil {
		if kerrors.IsNotFound(err) {
			log.Debug("user cluster external service not found")
			return nil
		}
		return fmt.Errorf("failed to get user cluster service: %v", err)
	}

	nodePort := getNodePort(outerService)
	oldService := service.DeepCopy()
	if nodePort != oldService.Spec.Ports[0].NodePort {
		oldService.Spec.Ports[0].NodePort = nodePort
		oldService.Spec.Ports[0].Port = nodePort
		oldService.Spec.Ports[0].TargetPort = intstr.FromInt(int(nodePort))
		if err := r.client.Update(r.ctx, oldService); err != nil {
			return fmt.Errorf("failed to update user cluster service: %v", err)
		}
		return nil
	}

	return nil
}

func (r *reconciler) createOuterService(pod *v1.Pod, jobID string) (*v1.Service, error) {
	newService := &v1.Service{
		ObjectMeta: metav1.ObjectMeta{
			Annotations: defaultServiceAnnotations,
			Name:        pod.Namespace,
			Namespace:   r.namespace,
			Labels:      map[string]string{"prow.k8s.io/id": jobID, "app": "cluster-exposer"},
		},
		Spec: v1.ServiceSpec{
			Selector: map[string]string{"prow.k8s.io/id": jobID},
			Type:     v1.ServiceTypeNodePort,
			Ports: []v1.ServicePort{
				{
					Name:     "secure",
					Port:     80,
					Protocol: v1.ProtocolTCP,
				},
			},
		},
	}
	err := r.outerClient.Create(r.ctx, newService)
	if err != nil {
		return nil, fmt.Errorf("failed to create outer service: %v", err)
	}

	// Update specified port to be the same as autogenerated NodePort
	nodePort := getNodePort(newService)
	newService.Spec.Ports[0].Port = nodePort
	newService.Spec.Ports[0].TargetPort = intstr.FromInt(int(nodePort))

	if err := r.outerClient.Update(r.ctx, newService); err != nil {
		return nil, fmt.Errorf("failed to update outer service: %v", err)
	}
	r.log.Debug("outer service created")
	return newService.DeepCopy(), nil
}

// clusterAPI returns a predicate func that only includes API pods in user cluster scope
func clusterAPI() predicate.Funcs {
	return Factory(func(m metav1.Object, r runtime.Object) bool {
		return strings.HasPrefix(m.GetName(), "apiserver-") && strings.HasPrefix(m.GetNamespace(), "cluster-")
	})
}

// Factory returns a predicate func that applies the given filter function
// on CREATE, UPDATE and DELETE events. For UPDATE events, the filter is applied
// to both the old and new object and OR's the result.
func Factory(filter func(m metav1.Object, r runtime.Object) bool) predicate.Funcs {
	return predicate.Funcs{
		CreateFunc: func(e event.CreateEvent) bool {
			return filter(e.Meta, e.Object)
		},
		UpdateFunc: func(e event.UpdateEvent) bool {
			return filter(e.MetaOld, e.ObjectOld) || filter(e.MetaNew, e.ObjectNew)
		},
		DeleteFunc: func(e event.DeleteEvent) bool {
			return filter(e.Meta, e.Object)
		},
	}
}

func getNodePort(service *v1.Service) int32 {
	for _, port := range service.Spec.Ports {
		if port.NodePort != 0 {
			return port.NodePort
		}
	}

	return 0
}
